{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa74798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os, math, string\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "# === NLTK SETUP ===\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"wordnet\")\n",
    "smooth_fn = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insert your token here\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dfc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42561050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7692/777344230.py:25: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n",
      "/tmp/ipykernel_7692/777344230.py:29: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21586\n",
      "20517\n",
      "14454\n",
      "shape: (11, 2)\n",
      "┌──────────────┬────────┐\n",
      "│ category     ┆ counts │\n",
      "│ ---          ┆ ---    │\n",
      "│ str          ┆ u32    │\n",
      "╞══════════════╪════════╡\n",
      "│ ACCOUNT      ┆ 3251   │\n",
      "│ ORDER        ┆ 2152   │\n",
      "│ REFUND       ┆ 1527   │\n",
      "│ SHIPPING     ┆ 1156   │\n",
      "│ DELIVERY     ┆ 1102   │\n",
      "│ …            ┆ …      │\n",
      "│ INVOICE      ┆ 1076   │\n",
      "│ PAYMENT      ┆ 1028   │\n",
      "│ FEEDBACK     ┆ 1004   │\n",
      "│ CANCEL       ┆ 539    │\n",
      "│ SUBSCRIPTION ┆ 537    │\n",
      "└──────────────┴────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7692/777344230.py:58: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  .agg(pl.count().alias(\"counts\"))\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "# Data cleaning\n",
    "# fill in null\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"response\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"intent\")\n",
    "      .cast(str)\n",
    "      .fill_null(\"unknown\")\n",
    "])\n",
    "\n",
    "# Remove emoji\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Remove all emojis safely\n",
    "\n",
    "# Apply to instruction and response\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\").map_elements(remove_emojis).alias(\"instruction\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"response\").map_elements(remove_emojis).alias(\"response\")\n",
    "])\n",
    "\n",
    "#Exclude noisy flags\n",
    "# Filter out rows where 'flags' contains Z, Q, or W ===\n",
    "flag = [\"flags\"]\n",
    "df_z = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\")\n",
    ")\n",
    "\n",
    "df_zw = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\")\n",
    ")\n",
    "\n",
    "df_clean = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Q\")\n",
    ")\n",
    "\n",
    "print(df_z.height)\n",
    "print(df_zw.height)\n",
    "print(df_clean.height)\n",
    "\n",
    "category_counts = (\n",
    "    df_clean\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.count().alias(\"counts\"))\n",
    "    .sort(\"counts\", descending=True)\n",
    ")\n",
    "\n",
    "print(category_counts)\n",
    "\n",
    "# Filter only selected categories\n",
    "selected_categories = [\"ORDER\", \"REFUND\", \"SHIPPING\", \"DELIVERY\"]\n",
    "df_selected = df_clean.filter(\n",
    "    pl.col(\"category\").is_in(selected_categories)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1043f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split sizes:\n",
      "Train: 4154\n",
      "Val:   890\n",
      "Test:  893\n",
      "\n",
      "📊 Category distribution in test set:\n",
      "shape: (4, 2)\n",
      "┌──────────┬───────┐\n",
      "│ category ┆ count │\n",
      "│ ---      ┆ ---   │\n",
      "│ str      ┆ u32   │\n",
      "╞══════════╪═══════╡\n",
      "│ DELIVERY ┆ 166   │\n",
      "│ ORDER    ┆ 323   │\n",
      "│ REFUND   ┆ 230   │\n",
      "│ SHIPPING ┆ 174   │\n",
      "└──────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# Split dataset by category\n",
    "# === Configuration ===\n",
    "LABEL_COL = \"category\"  # 🔁 Replace with \"intent\" or any stratification column\n",
    "SPLIT_RATIO_TRAIN = 0.7\n",
    "SPLIT_RATIO_VAL = 0.15\n",
    "SEED = 123\n",
    "df_final = df_selected.clone()\n",
    "\n",
    "# === Stratified split logic ===\n",
    "random.seed(SEED)\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "val_parts = []\n",
    "\n",
    "for label in df_final[LABEL_COL].unique().to_list():\n",
    "    group_df = df_final.filter(pl.col(LABEL_COL) == label)\n",
    "    group_df = group_df.sample(n=len(group_df), shuffle=True, seed=SEED)\n",
    "\n",
    "    n = len(group_df)\n",
    "    train_idx = int(n * SPLIT_RATIO_TRAIN)\n",
    "    val_idx = int(n * (SPLIT_RATIO_TRAIN + SPLIT_RATIO_VAL))\n",
    "\n",
    "    train_parts.append(group_df[:train_idx])\n",
    "    val_parts.append(group_df[train_idx:val_idx])\n",
    "    test_parts.append(group_df[val_idx:])\n",
    "\n",
    "# === Combine all groups\n",
    "train_df = pl.concat(train_parts).sort([\"category\", \"instruction\"])\n",
    "val_df = pl.concat(val_parts).sort([\"category\", \"instruction\"])\n",
    "test_df = pl.concat(test_parts).sort([\"category\", \"instruction\"])\n",
    "\n",
    "print(\"✅ Split sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(\"\\n📊 Category distribution in test set:\")\n",
    "print(test_df.select([pl.col(LABEL_COL)]).to_series().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177edb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running inference for: allenai/OLMo-2-0425-1B + prompt-tune-outputs/OLMo-2-0425-1B-faq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35de0662a272492980ff75fff3587223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resuming from 854 completed rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLMO Inference: 100%|██████████| 893/893 [00:00<00:00, 1256462.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done with olmo. Results saved to chatbot_result/prompt_tune_ft/scenario_C_prompt_tune_ft_results_olmo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "scenario_no = \"C\"\n",
    "scenario = \"prompt_tune_ft\"   # <- was \"qlora_ft\"\n",
    "\n",
    "MODELS = {\n",
    "    # \"llama\": {\n",
    "    #     \"base\": \"meta-llama/Llama-3.2-1B\",\n",
    "    #     \"adapter\": \"prompt-tune-outputs/Llama-3.2-1B-faq\",   # <- your PROMPT-TUNING adapter folder\n",
    "    # },\n",
    "    # add more prompt-tuned models if you want:\n",
    "    # \"qwen\": {\n",
    "    #     \"base\": \"Qwen/Qwen3-0.6B-Base\",\n",
    "    #     \"adapter\": \"prompt-tune-outputs/Qwen3-0.6B-Base-faq\",\n",
    "    # },\n",
    "    \"olmo\": {\n",
    "        \"base\": \"allenai/OLMo-2-0425-1B\",\n",
    "        \"adapter\": \"prompt-tune-outputs/OLMo-2-0425-1B-faq\",\n",
    "    },\n",
    "}\n",
    "\n",
    "OFFLOAD_DIR = \"./offload\"\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "SAVE_EVERY = 20\n",
    "\n",
    "# === Load test set (pre-split externally)\n",
    "questions = test_df[\"instruction\"].cast(str).to_list()\n",
    "answers = test_df[\"response\"].cast(str).to_list()\n",
    "\n",
    "# === Safe append function\n",
    "def append_to_csv(new_data: pl.DataFrame, path: str):\n",
    "    if os.path.exists(path):\n",
    "        existing = pl.read_csv(path)\n",
    "        combined = existing.vstack(new_data)\n",
    "    else:\n",
    "        combined = new_data\n",
    "    combined.write_csv(path)\n",
    "\n",
    "def adaptive_max_new_tokens(gold_text: str, tokenizer,\n",
    "                            min_nt=48, max_nt=300, mult=1.3, bonus=16) -> int:\n",
    "    # estimate needed length from gold answer tokens\n",
    "    n_gold = len(tokenizer(gold_text, add_special_tokens=False).input_ids)\n",
    "    est = int(round(mult * n_gold) + bonus)   # headroom\n",
    "    return int(np.clip(est, min_nt, max_nt))\n",
    "\n",
    "\n",
    "def load_tokenizer(model_or_path: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_or_path, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "    return tok\n",
    "\n",
    "def load_base_4bit(base_name: str, offload_dir: str):\n",
    "    supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "    bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if supports_bf16 else torch.float16,\n",
    "    )\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        base_name,\n",
    "        quantization_config=bnb,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "        torch_dtype=torch.bfloat16 if supports_bf16 else torch.float16,\n",
    "        offload_folder=offload_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "def enable_inference_cache(model):\n",
    "    if getattr(model.config, \"use_cache\", None) is not True:\n",
    "        model.config.use_cache = True\n",
    "\n",
    "# === Loop through each model\n",
    "for key, cfg in MODELS.items():\n",
    "    BASE = cfg[\"base\"]\n",
    "    ADAPTER_DIR = cfg[\"adapter\"]\n",
    "\n",
    "    OUTPUT_CSV = f\"chatbot_result/{scenario}/scenario_{scenario_no}_{scenario}_results_{key}.csv\"\n",
    "    print(f\"\\n🚀 Running inference for: {BASE} + {ADAPTER_DIR}\")\n",
    "    os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # --- Load base (4-bit) + Prompt-Tuning adapter ---\n",
    "    tokenizer = load_tokenizer(BASE)\n",
    "    base_model = load_base_4bit(BASE, OFFLOAD_DIR)\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "    enable_inference_cache(model)\n",
    "    model.eval()\n",
    "\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "    # Resume support\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        existing = pl.read_csv(OUTPUT_CSV)\n",
    "        done = set(existing[\"instruction\"].to_list())\n",
    "        print(f\"🔄 Resuming from {len(done)} completed rows...\")\n",
    "    else:\n",
    "        done = set()\n",
    "\n",
    "    results = []\n",
    "    for question, ground_truth in tqdm(zip(questions, answers), total=len(questions), desc=f\"{key.upper()} Inference\"):\n",
    "        if question in done:\n",
    "            continue\n",
    "\n",
    "        prompt = f\"\"\"You are a helpful retail assistant. Answer the following customer query briefly and accurately.\\n\\nCustomer: {question}\\nAnswer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            max_nt = adaptive_max_new_tokens(ground_truth, tokenizer,\n",
    "                                            min_nt=48, max_nt=300, mult=1.3, bonus=16)\n",
    "\n",
    "            out = pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=max_nt,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )[0][\"generated_text\"]\n",
    "\n",
    "            # keep the part after the LAST \"Answer:\" (more robust to echoes)\n",
    "            predicted = out.split(\"Answer:\")[-1].strip() if \"Answer:\" in out else out.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing question: {question[:60]}... Skipping. Reason: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append((question, ground_truth, predicted))\n",
    "\n",
    "        if len(results) >= SAVE_EVERY:\n",
    "            chunk_df = pl.DataFrame(results, schema=[\"instruction\", \"response\", \"prediction\"])\n",
    "            enriched = test_df.join(chunk_df, on=[\"instruction\", \"response\"], how=\"inner\")\n",
    "            append_to_csv(enriched, OUTPUT_CSV)\n",
    "            results = []\n",
    "\n",
    "    if results:\n",
    "        chunk_df = pl.DataFrame(results, schema=[\"instruction\", \"response\", \"prediction\"])\n",
    "        enriched = test_df.join(chunk_df, on=[\"instruction\", \"response\"], how=\"inner\")\n",
    "        append_to_csv(enriched, OUTPUT_CSV)\n",
    "\n",
    "    print(f\"✅ Done with {key}. Results saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4330cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running Evaluation for: meta-llama/Llama-3.2-1B\n",
      "\n",
      "🚀 Running Evaluation for: Qwen/Qwen3-0.6B-Base\n",
      "\n",
      "🚀 Running Evaluation for: allenai/OLMo-2-0425-1B\n",
      "\n",
      "📊 Combined (pivot) comparison:\n",
      "shape: (5, 4)\n",
      "┌──────────────┬───────┬───────┬───────┐\n",
      "│ Metric       ┆ llama ┆ qwen  ┆ olmo  │\n",
      "│ ---          ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ str          ┆ f64   ┆ f64   ┆ f64   │\n",
      "╞══════════════╪═══════╪═══════╪═══════╡\n",
      "│ F1-overlap   ┆ 32.02 ┆ 28.39 ┆ 18.49 │\n",
      "│ BLEU         ┆ 12.65 ┆ 8.54  ┆ 3.39  │\n",
      "│ ROUGE-L      ┆ 20.82 ┆ 20.09 ┆ 13.64 │\n",
      "│ METEOR       ┆ 22.11 ┆ 16.26 ┆ 9.75  │\n",
      "│ BERTScore-F1 ┆ 21.83 ┆ 20.79 ┆ 12.34 │\n",
      "└──────────────┴───────┴───────┴───────┘\n",
      "\n",
      "✅ Saved under 'Evaluation/'\n",
      "- Scenario_C_Evaluation_AllModels.csv\n",
      "- Scenario_C_Evaluation_Pivot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7692/600669144.py:134: DeprecationWarning: the argument `columns` for `DataFrame.pivot` is deprecated. It was renamed to `on` in version 1.0.0.\n",
      "  pivot_df = final_df.pivot(index=\"Metric\", columns=\"Model\", values=\"Score\")\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "scenario_no = \"C\"\n",
    "scenario = \"prompt_tune_ft\"\n",
    "DEVICE = \"cuda\"\n",
    "MODELS = {\n",
    "    \"llama\":    \"meta-llama/Llama-3.2-1B\",\n",
    "    \"qwen\":     \"Qwen/Qwen3-0.6B-Base\",\n",
    "    \"olmo\":     \"allenai/OLMo-2-0425-1B\",\n",
    "}\n",
    "\n",
    "BERTSCORE_MODEL = \"microsoft/deberta-base-mnli\"  # fast & solid for FAQ/QA\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = \"Evaluation\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# BLEU smoothing (method4 is friendlier for short answers)\n",
    "smooth_fn = SmoothingFunction().method4\n",
    "# Cache rouge scorer once\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# === Helpers ===\n",
    "def _safe_text(x):\n",
    "    return \"\" if x is None or (isinstance(x, float) and math.isnan(x)) else str(x)\n",
    "\n",
    "def normalize(text: str) -> list[str]:\n",
    "    text = _safe_text(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "# def exact_match(pred: str, gold: str) -> int:\n",
    "#     return int(_safe_text(pred).strip().lower() == _safe_text(gold).strip().lower())\n",
    "\n",
    "def f1_score_overlap(pred: str, gold: str) -> float:\n",
    "    pred_tokens = normalize(pred)\n",
    "    gold_tokens = normalize(gold)\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def bleu_score(pred: str, gold: str) -> float:\n",
    "    # BLEU-2 is fairer for short FAQ answers\n",
    "    return sentence_bleu(\n",
    "        [normalize(gold)], normalize(pred),\n",
    "        weights=(0.5, 0.5, 0, 0),\n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "\n",
    "def rouge_l_score(pred: str, gold: str) -> float:\n",
    "    return rouge.score(_safe_text(gold), _safe_text(pred))[\"rougeL\"].fmeasure\n",
    "\n",
    "def meteor(pred: str, gold: str) -> float:\n",
    "    return meteor_score([normalize(gold)], normalize(pred))\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = _safe_text(t).strip()\n",
    "    for pre in (\"answer:\", \"response:\", \"reply:\"):\n",
    "        if t.lower().startswith(pre):\n",
    "            t = t[len(pre):].strip()\n",
    "    return t.strip(\"`\").strip()\n",
    "\n",
    "def compute_metrics(preds_raw, golds_raw):\n",
    "    preds = [clean_text(p) for p in preds_raw]\n",
    "    golds = [clean_text(g) for g in golds_raw]\n",
    "\n",
    "    # em_list, f1_list, bleu_list, rouge_list, meteor_list = [], [], [], [], []\n",
    "    f1_list, bleu_list, rouge_list, meteor_list = [], [], [], []\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        # em_list.append(exact_match(pred, gold))\n",
    "        f1_list.append(f1_score_overlap(pred, gold))\n",
    "        bleu_list.append(bleu_score(pred, gold))\n",
    "        rouge_list.append(rouge_l_score(pred, gold))\n",
    "        meteor_list.append(meteor(pred, gold))\n",
    "\n",
    "    # BERTScore on CUDA; drop empty pairs to avoid warnings\n",
    "    mask = [bool(p.strip()) and bool(g.strip()) for p, g in zip(preds, golds)]\n",
    "    preds_nz = [p for p, m in zip(preds, mask) if m]\n",
    "    golds_nz = [g for g, m in zip(golds, mask) if m]\n",
    "    if len(preds_nz) == 0:\n",
    "        bert_f1_avg = 0.0\n",
    "    else:\n",
    "        P, R, F1 = bertscore(\n",
    "            preds_nz, golds_nz,\n",
    "            lang=\"en\",\n",
    "            model_type=BERTSCORE_MODEL,\n",
    "            rescale_with_baseline=True,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        bert_f1_avg = float(F1.mean()) * 100.0\n",
    "\n",
    "    n = max(1, len(f1_list))\n",
    "    metrics = [\n",
    "        # \"Exact Match\", \n",
    "        \"F1-overlap\", \"BLEU\", \"ROUGE-L\", \"METEOR\", \"BERTScore-F1\"]\n",
    "    scores = [\n",
    "        # round(sum(em_list) / n * 100, 2),\n",
    "        round(sum(f1_list) / n * 100, 2),\n",
    "        round(sum(bleu_list) / n * 100, 2),\n",
    "        round(sum(rouge_list) / n * 100, 2),\n",
    "        round(sum(meteor_list) / n * 100, 2),\n",
    "        round(bert_f1_avg, 2),\n",
    "    ]\n",
    "    assert len(metrics) == len(scores), f\"metrics={len(metrics)} scores={len(scores)}\"\n",
    "\n",
    "    rows = [{\"Metric\": m, \"Score\": s} for m, s in zip(metrics, scores)]\n",
    "    return pl.DataFrame(rows)\n",
    "\n",
    "# === Run all models and combine outputs ===\n",
    "all_summaries = []\n",
    "\n",
    "for key, MODEL_NAME in MODELS.items():\n",
    "    OUTPUT_CSV = f\"chatbot_result/{scenario}/scenario_{scenario_no}_{scenario}_results_{key}.csv\"\n",
    "\n",
    "    print(f\"\\n🚀 Running Evaluation for: {MODEL_NAME}\")\n",
    "\n",
    "    result_df = pl.read_csv(OUTPUT_CSV)\n",
    "    preds = result_df[\"prediction\"].to_list()\n",
    "    golds = result_df[\"response\"].to_list()\n",
    "\n",
    "    summary = compute_metrics(preds, golds).with_columns(pl.lit(key).alias(\"Model\"))\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "# Combine all models (long)\n",
    "if all_summaries:\n",
    "    final_df = pl.concat(all_summaries).select([\"Model\", \"Metric\", \"Score\"])\n",
    "    final_df.write_csv(os.path.join(OUT_DIR, f\"Scenario_{scenario_no}_Evaluation_AllModels.csv\"))\n",
    "\n",
    "    # Pivot for side-by-side comparison\n",
    "    pivot_df = final_df.pivot(index=\"Metric\", columns=\"Model\", values=\"Score\")\n",
    "    pivot_df.write_csv(os.path.join(OUT_DIR, f\"Scenario_{scenario_no}_Evaluation_Pivot.csv\"))\n",
    "\n",
    "    print(\"\\n📊 Combined (pivot) comparison:\")\n",
    "    print(pivot_df)\n",
    "    print(\"\\n✅ Saved under 'Evaluation/'\")\n",
    "    print(f\"- Scenario_{scenario_no}_Evaluation_AllModels.csv\")\n",
    "    print(f\"- Scenario_{scenario_no}_Evaluation_Pivot.csv\")\n",
    "else:\n",
    "    print(\"⚠️ No model results were processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
