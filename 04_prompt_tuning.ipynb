{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ba09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"         # single GPU, avoids DataParallel\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import gc\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PromptTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71035986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insert your token here\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc9f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636c6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700/292506220.py:26: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n",
      "/tmp/ipykernel_700/292506220.py:30: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21586\n",
      "20517\n",
      "14454\n",
      "shape: (11, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ category     ‚îÜ counts ‚îÇ\n",
      "‚îÇ ---          ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str          ‚îÜ u32    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ ACCOUNT      ‚îÜ 3251   ‚îÇ\n",
      "‚îÇ ORDER        ‚îÜ 2152   ‚îÇ\n",
      "‚îÇ REFUND       ‚îÜ 1527   ‚îÇ\n",
      "‚îÇ SHIPPING     ‚îÜ 1156   ‚îÇ\n",
      "‚îÇ DELIVERY     ‚îÜ 1102   ‚îÇ\n",
      "‚îÇ ‚Ä¶            ‚îÜ ‚Ä¶      ‚îÇ\n",
      "‚îÇ INVOICE      ‚îÜ 1076   ‚îÇ\n",
      "‚îÇ PAYMENT      ‚îÜ 1028   ‚îÇ\n",
      "‚îÇ FEEDBACK     ‚îÜ 1004   ‚îÇ\n",
      "‚îÇ CANCEL       ‚îÜ 539    ‚îÇ\n",
      "‚îÇ SUBSCRIPTION ‚îÜ 537    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "‚úÖ Split sizes:\n",
      "Train: 4154\n",
      "Val:   890\n",
      "Test:  893\n",
      "\n",
      "üìä Category distribution in test set:\n",
      "shape: (4, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ category ‚îÜ count ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---   ‚îÇ\n",
      "‚îÇ str      ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ DELIVERY ‚îÜ 166   ‚îÇ\n",
      "‚îÇ ORDER    ‚îÜ 323   ‚îÇ\n",
      "‚îÇ REFUND   ‚îÜ 230   ‚îÇ\n",
      "‚îÇ SHIPPING ‚îÜ 174   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700/292506220.py:59: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  .agg(pl.count().alias(\"counts\"))\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "\n",
    "# Data cleaning\n",
    "# fill in null\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"response\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"intent\")\n",
    "      .cast(str)\n",
    "      .fill_null(\"unknown\")\n",
    "])\n",
    "\n",
    "# Remove emoji\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Remove all emojis safely\n",
    "\n",
    "# Apply to instruction and response\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\").map_elements(remove_emojis).alias(\"instruction\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"response\").map_elements(remove_emojis).alias(\"response\")\n",
    "])\n",
    "\n",
    "#Exclude noisy flags\n",
    "# Filter out rows where 'flags' contains Z, Q, or W ===\n",
    "flag = [\"flags\"]\n",
    "df_z = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\")\n",
    ")\n",
    "\n",
    "df_zw = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\")\n",
    ")\n",
    "\n",
    "df_clean = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Q\")\n",
    ")\n",
    "\n",
    "print(df_z.height)\n",
    "print(df_zw.height)\n",
    "print(df_clean.height)\n",
    "\n",
    "category_counts = (\n",
    "    df_clean\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.count().alias(\"counts\"))\n",
    "    .sort(\"counts\", descending=True)\n",
    ")\n",
    "\n",
    "print(category_counts)\n",
    "\n",
    "# Filter only selected categories\n",
    "selected_categories = [\"ORDER\", \"REFUND\", \"SHIPPING\", \"DELIVERY\"]\n",
    "df_selected = df_clean.filter(\n",
    "    pl.col(\"category\").is_in(selected_categories)\n",
    ")\n",
    "\n",
    "\n",
    "# Split dataset by category\n",
    "# === Configuration ===\n",
    "LABEL_COL = \"category\"  # üîÅ Replace with \"intent\" or any stratification column\n",
    "SPLIT_RATIO_TRAIN = 0.7\n",
    "SPLIT_RATIO_VAL = 0.15\n",
    "SEED = 123\n",
    "df_final = df_selected.clone()\n",
    "\n",
    "# === Stratified split logic ===\n",
    "random.seed(SEED)\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "val_parts = []\n",
    "\n",
    "for label in df_final[LABEL_COL].unique().to_list():\n",
    "    group_df = df_final.filter(pl.col(LABEL_COL) == label)\n",
    "    group_df = group_df.sample(n=len(group_df), shuffle=True, seed=SEED)\n",
    "\n",
    "    n = len(group_df)\n",
    "    train_idx = int(n * SPLIT_RATIO_TRAIN)\n",
    "    val_idx = int(n * (SPLIT_RATIO_TRAIN + SPLIT_RATIO_VAL))\n",
    "\n",
    "    train_parts.append(group_df[:train_idx])\n",
    "    val_parts.append(group_df[train_idx:val_idx])\n",
    "    test_parts.append(group_df[val_idx:])\n",
    "\n",
    "# === Combine all groups\n",
    "train_df = pl.concat(train_parts).sort([\"category\", \"instruction\"])\n",
    "val_df = pl.concat(val_parts).sort([\"category\", \"instruction\"])\n",
    "test_df = pl.concat(test_parts).sort([\"category\", \"instruction\"])\n",
    "\n",
    "print(\"‚úÖ Split sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(\"\\nüìä Category distribution in test set:\")\n",
    "print(test_df.select([pl.col(LABEL_COL)]).to_series().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56886f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Prompt Tuning for [olmo] allenai/OLMo-2-0425-1B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29334b6c690417f9f11790ec1f231e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 61,440 || all params: 1,484,978,176 || trainable%: 0.0041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fe4e33ed984e82a38da41aac24b09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3401f9cc43d14f1bbc3e1294197e78a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Resuming from: prompt-tune-outputs/OLMo-2-0425-1B-faq/checkpoint-1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3250' max='3250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3250/3250 20:16:27, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.381800</td>\n",
       "      <td>1.338635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.374700</td>\n",
       "      <td>1.332626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.370100</td>\n",
       "      <td>1.326791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.364300</td>\n",
       "      <td>1.322115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.359100</td>\n",
       "      <td>1.317907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.354700</td>\n",
       "      <td>1.313043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.308115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.346800</td>\n",
       "      <td>1.304080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.343000</td>\n",
       "      <td>1.300868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.339900</td>\n",
       "      <td>1.296871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.336100</td>\n",
       "      <td>1.294265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.333500</td>\n",
       "      <td>1.291453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>1.288732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.328200</td>\n",
       "      <td>1.285891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.325600</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.323400</td>\n",
       "      <td>1.281840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.321800</td>\n",
       "      <td>1.279989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.319700</td>\n",
       "      <td>1.277729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.318000</td>\n",
       "      <td>1.276395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.316400</td>\n",
       "      <td>1.275110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.315700</td>\n",
       "      <td>1.273451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.313600</td>\n",
       "      <td>1.272706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.313100</td>\n",
       "      <td>1.271317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.312000</td>\n",
       "      <td>1.270811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.311100</td>\n",
       "      <td>1.269709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1.269174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.309600</td>\n",
       "      <td>1.268824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.309300</td>\n",
       "      <td>1.268221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1.268121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.308600</td>\n",
       "      <td>1.267902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved Prompt Tuning adapter to: prompt-tune-outputs/OLMo-2-0425-1B-faq\n",
      "\n",
      "üéâ All selected models prompt-tuned.\n"
     ]
    }
   ],
   "source": [
    "# ---------- sanity & speed ----------\n",
    "assert torch.cuda.is_available(), \"‚ùå CUDA is required for Prompt Tuning\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "set_seed(123)\n",
    "\n",
    "# ---------- your models ----------\n",
    "MODELS = {\n",
    "    # \"llama\":    \"meta-llama/Llama-3.2-1B\",\n",
    "    # \"qwen\":     \"Qwen/Qwen3-0.6B-Base\",\n",
    "    \"olmo\":     \"allenai/OLMo-2-0425-1B\"\n",
    "}\n",
    "OUT_ROOT = \"prompt-tune-outputs\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- dataset builder ----------\n",
    "def build_train_dataset(df: pl.DataFrame) -> Dataset:\n",
    "    df_text = (\n",
    "        df.select([\"instruction\", \"response\"])\n",
    "          .drop_nulls([\"instruction\", \"response\"])\n",
    "          .with_columns(\n",
    "              pl.col(\"instruction\").cast(pl.Utf8),\n",
    "              pl.col(\"response\").cast(pl.Utf8),\n",
    "          )\n",
    "          .with_columns(\n",
    "              pl.format(\n",
    "                  \"You are a helpful retail assistant. Answer the following customer query briefly and accurately.\\n\\nCustomer: {}\\nAnswer: {}\",\n",
    "                  pl.col(\"instruction\"),\n",
    "                  pl.col(\"response\"),\n",
    "                #   pl.lit(tokenizer.eos_token)   # <-- add eos\n",
    "              ).alias(\"text\")\n",
    "          )\n",
    "          .select([\"text\"])\n",
    "    )\n",
    "    return Dataset.from_dict({\"text\": df_text[\"text\"].to_list()})\n",
    "\n",
    "\n",
    "train_dataset = build_train_dataset(train_df)\n",
    "val_dataset   = build_train_dataset(val_df) if ('val_df' in globals() and val_df is not None) else None\n",
    "\n",
    "# ---------- trainer settings ----------\n",
    "def make_training_args(out_dir: str, supports_bf16: bool) -> TrainingArguments:\n",
    "    return TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=32,\n",
    "        learning_rate=5e-4,                  # often higher than LoRA\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        prediction_loss_only=True,\n",
    "        bf16=supports_bf16,\n",
    "        fp16=not supports_bf16,\n",
    "        gradient_checkpointing=True,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "def train_one_model(model_name: str, key: str):\n",
    "    print(f\"\\nüöÄ Training Prompt Tuning for [{key}] {model_name}\")\n",
    "\n",
    "    # bf16 support check\n",
    "    dev = torch.cuda.current_device()\n",
    "    supports_bf16 = torch.cuda.get_device_capability(dev)[0] >= 8\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # base model (frozen)\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if supports_bf16 else torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base.config.use_cache = False  # needed with checkpointing\n",
    "\n",
    "    # prepare prompt tuning config\n",
    "    prompt_cfg = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        num_virtual_tokens=30,       # size of learned soft prompt\n",
    "        tokenizer_name_or_path=model_name,\n",
    "    )\n",
    "    peft_model = get_peft_model(base, prompt_cfg)\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # trainer setup\n",
    "    out_dir = os.path.join(OUT_ROOT, f\"{model_name.split('/')[-1]}-faq\")\n",
    "    args = make_training_args(out_dir, supports_bf16)\n",
    "\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "    train_tok = train_dataset.map(_tok, batched=True, remove_columns=train_dataset.column_names)\n",
    "    val_tok   = val_dataset.map(_tok, batched=True, remove_columns=val_dataset.column_names) if val_dataset else None\n",
    "\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        data_collator=collator,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.00001\n",
    "        )],\n",
    "    )\n",
    "\n",
    "    # continue from previous training\n",
    "    last_ckpt = None\n",
    "    if os.path.isdir(out_dir):\n",
    "        ckpts = [os.path.join(out_dir, d) for d in os.listdir(out_dir) if d.startswith(\"checkpoint\")]\n",
    "        if ckpts:\n",
    "            last_ckpt = max(ckpts, key=os.path.getmtime)\n",
    "            print(f\"üîÅ Resuming from: {last_ckpt}\")\n",
    "\n",
    "    # train & save\n",
    "    # continue training\n",
    "    trainer.train(resume_from_checkpoint=last_ckpt) \n",
    "    # trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    print(f\"‚úÖ Saved Prompt Tuning adapter to: {out_dir}\")\n",
    "\n",
    "    # save logs\n",
    "    df_logs = pl.DataFrame(trainer.state.log_history)\n",
    "    df_logs.write_csv(os.path.join(out_dir, \"train_eval_log.csv\"))\n",
    "\n",
    "    # cleanup\n",
    "    del trainer, peft_model, base, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# ---------- loop over models ----------\n",
    "for key, model_name in MODELS.items():\n",
    "    try:\n",
    "        train_one_model(model_name, key)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Skipping [{key}] due to error: {e}\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        continue\n",
    "\n",
    "print(\"\\nüéâ All selected models prompt-tuned.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
