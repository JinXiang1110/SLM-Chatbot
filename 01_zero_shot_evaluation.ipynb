{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce9af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os, math, string\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "# === NLTK SETUP ===\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"wordnet\")\n",
    "smooth_fn = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insert your token here\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e263d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d57ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddff766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1170/928729037.py:24: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n",
      "/tmp/ipykernel_1170/928729037.py:28: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df = df.with_columns([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21586\n",
      "20517\n",
      "14454\n",
      "shape: (11, 2)\n",
      "┌──────────────┬────────┐\n",
      "│ category     ┆ counts │\n",
      "│ ---          ┆ ---    │\n",
      "│ str          ┆ u32    │\n",
      "╞══════════════╪════════╡\n",
      "│ ACCOUNT      ┆ 3251   │\n",
      "│ ORDER        ┆ 2152   │\n",
      "│ REFUND       ┆ 1527   │\n",
      "│ SHIPPING     ┆ 1156   │\n",
      "│ DELIVERY     ┆ 1102   │\n",
      "│ …            ┆ …      │\n",
      "│ INVOICE      ┆ 1076   │\n",
      "│ PAYMENT      ┆ 1028   │\n",
      "│ FEEDBACK     ┆ 1004   │\n",
      "│ CANCEL       ┆ 539    │\n",
      "│ SUBSCRIPTION ┆ 537    │\n",
      "└──────────────┴────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1170/928729037.py:57: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  .agg(pl.count().alias(\"counts\"))\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "# fill in null\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"response\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"intent\")\n",
    "      .cast(str)\n",
    "      .fill_null(\"unknown\")\n",
    "])\n",
    "\n",
    "# Remove emoji\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Remove all emojis safely\n",
    "\n",
    "# Apply to instruction and response\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\").map_elements(remove_emojis).alias(\"instruction\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"response\").map_elements(remove_emojis).alias(\"response\")\n",
    "])\n",
    "\n",
    "#Exclude noisy flags\n",
    "# Filter out rows where 'flags' contains Z, Q, or W ===\n",
    "flag = [\"flags\"]\n",
    "df_z = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\")\n",
    ")\n",
    "\n",
    "df_zw = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\")\n",
    ")\n",
    "\n",
    "df_clean = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Q\")\n",
    ")\n",
    "\n",
    "print(df_z.height)\n",
    "print(df_zw.height)\n",
    "print(df_clean.height)\n",
    "\n",
    "category_counts = (\n",
    "    df_clean\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.count().alias(\"counts\"))\n",
    "    .sort(\"counts\", descending=True)\n",
    ")\n",
    "\n",
    "print(category_counts)\n",
    "\n",
    "# Filter only selected categories\n",
    "selected_categories = [\"ORDER\", \"REFUND\", \"SHIPPING\", \"DELIVERY\"]\n",
    "df_selected = df_clean.filter(\n",
    "    pl.col(\"category\").is_in(selected_categories)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd50571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split sizes:\n",
      "Train: 4154\n",
      "Val:   890\n",
      "Test:  893\n",
      "\n",
      "📊 Category distribution in test set:\n",
      "shape: (4, 2)\n",
      "┌──────────┬───────┐\n",
      "│ category ┆ count │\n",
      "│ ---      ┆ ---   │\n",
      "│ str      ┆ u32   │\n",
      "╞══════════╪═══════╡\n",
      "│ DELIVERY ┆ 166   │\n",
      "│ ORDER    ┆ 323   │\n",
      "│ REFUND   ┆ 230   │\n",
      "│ SHIPPING ┆ 174   │\n",
      "└──────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# Split dataset by category\n",
    "# === Configuration ===\n",
    "LABEL_COL = \"category\"  # 🔁 Replace with \"intent\" or any stratification column\n",
    "SPLIT_RATIO_TRAIN = 0.7\n",
    "SPLIT_RATIO_VAL = 0.15\n",
    "SEED = 123\n",
    "df_final = df_selected.clone()\n",
    "\n",
    "# === Stratified split logic ===\n",
    "random.seed(SEED)\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "val_parts = []\n",
    "\n",
    "for label in df_final[LABEL_COL].unique().to_list():\n",
    "    group_df = df_final.filter(pl.col(LABEL_COL) == label)\n",
    "    group_df = group_df.sample(n=len(group_df), shuffle=True, seed=SEED)\n",
    "\n",
    "    n = len(group_df)\n",
    "    train_idx = int(n * SPLIT_RATIO_TRAIN)\n",
    "    val_idx = int(n * (SPLIT_RATIO_TRAIN + SPLIT_RATIO_VAL))\n",
    "\n",
    "    train_parts.append(group_df[:train_idx])\n",
    "    val_parts.append(group_df[train_idx:val_idx])\n",
    "    test_parts.append(group_df[val_idx:])\n",
    "\n",
    "# === Combine all groups\n",
    "train_df = pl.concat(train_parts).sort([\"category\", \"instruction\"])\n",
    "val_df = pl.concat(val_parts).sort([\"category\", \"instruction\"])\n",
    "test_df = pl.concat(test_parts).sort([\"category\", \"instruction\"])\n",
    "\n",
    "print(\"✅ Split sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(\"\\n📊 Category distribution in test set:\")\n",
    "print(test_df.select([pl.col(LABEL_COL)]).to_series().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5207f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running inference for: meta-llama/Llama-3.2-1B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "LLAMA Inference:   0%|          | 0/893 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   0%|          | 1/893 [00:00<12:48,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   0%|          | 2/893 [00:05<42:52,  2.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   0%|          | 3/893 [00:09<53:53,  3.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   0%|          | 4/893 [00:11<44:33,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 5/893 [00:14<41:49,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 6/893 [00:17<44:32,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 7/893 [00:21<49:00,  3.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 8/893 [00:23<42:20,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 9/893 [00:25<38:23,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 10/893 [00:27<35:30,  2.41s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|          | 11/893 [00:29<32:14,  2.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|▏         | 12/893 [00:29<24:08,  1.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   1%|▏         | 13/893 [00:31<25:10,  1.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 14/893 [00:36<40:35,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 15/893 [00:37<31:46,  2.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 16/893 [00:39<30:04,  2.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 17/893 [00:41<29:15,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 18/893 [00:44<34:14,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 19/893 [00:46<34:57,  2.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/zorin17/miniconda3/envs/llm/lib/python3.10/functools.py:889: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return dispatch(args[0].__class__)(*args, **kw)\n",
      "LLAMA Inference:   2%|▏         | 20/893 [00:51<46:32,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 21/893 [00:53<40:56,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   2%|▏         | 22/893 [00:57<46:24,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 23/893 [00:59<37:56,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 24/893 [01:03<46:44,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 25/893 [01:09<55:45,  3.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 26/893 [01:14<1:00:41,  4.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 27/893 [01:17<57:27,  3.98s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 28/893 [01:20<53:32,  3.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 29/893 [01:23<48:15,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 30/893 [01:25<44:23,  3.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   3%|▎         | 31/893 [01:29<46:28,  3.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▎         | 32/893 [01:30<39:57,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▎         | 33/893 [01:31<28:44,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 34/893 [01:33<28:26,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 35/893 [01:37<37:07,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 36/893 [01:37<28:17,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 37/893 [01:40<30:26,  2.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 38/893 [01:41<28:49,  2.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 39/893 [01:47<42:33,  2.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   4%|▍         | 40/893 [01:49<38:12,  2.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▍         | 41/893 [01:50<34:00,  2.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▍         | 42/893 [01:55<42:17,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▍         | 43/893 [02:00<51:56,  3.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▍         | 44/893 [02:00<38:33,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▌         | 45/893 [02:04<40:32,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▌         | 46/893 [02:04<29:25,  2.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▌         | 47/893 [02:04<21:39,  1.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▌         | 48/893 [02:06<23:38,  1.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   5%|▌         | 49/893 [02:09<27:47,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 50/893 [02:10<23:44,  1.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 51/893 [02:10<17:41,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 52/893 [02:13<26:20,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 53/893 [02:18<36:26,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 54/893 [02:21<38:13,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▌         | 55/893 [02:23<34:31,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▋         | 56/893 [02:25<32:39,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▋         | 57/893 [02:30<45:14,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   6%|▋         | 58/893 [02:35<53:34,  3.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 59/893 [02:37<45:37,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 60/893 [02:39<39:56,  2.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 61/893 [02:41<35:55,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 62/893 [02:44<36:56,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 63/893 [02:48<44:00,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 64/893 [02:51<42:17,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 65/893 [02:55<45:30,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   7%|▋         | 66/893 [02:58<44:48,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 67/893 [03:00<40:20,  2.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 68/893 [03:01<29:04,  2.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 69/893 [03:05<38:30,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 70/893 [03:08<39:34,  2.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 71/893 [03:13<48:49,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 72/893 [03:16<47:00,  3.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 73/893 [03:17<34:20,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 74/893 [03:20<37:33,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   8%|▊         | 75/893 [03:23<38:40,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▊         | 76/893 [03:27<43:17,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▊         | 77/893 [03:32<50:23,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▊         | 78/893 [03:37<54:19,  4.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 79/893 [03:37<40:28,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 80/893 [03:41<45:22,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 81/893 [03:43<39:13,  2.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 82/893 [03:45<36:16,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 83/893 [03:50<45:08,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:   9%|▉         | 84/893 [03:52<39:10,  2.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|▉         | 85/893 [03:52<28:09,  2.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|▉         | 86/893 [03:54<27:47,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|▉         | 87/893 [03:56<26:37,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|▉         | 88/893 [04:01<39:01,  2.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|▉         | 89/893 [04:05<44:09,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|█         | 90/893 [04:10<47:29,  3.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|█         | 91/893 [04:14<50:46,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|█         | 92/893 [04:18<53:17,  3.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  10%|█         | 93/893 [04:20<45:16,  3.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 94/893 [04:22<39:24,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 95/893 [04:24<35:01,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 96/893 [04:29<44:31,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 97/893 [04:31<38:28,  2.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 98/893 [04:35<44:16,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 99/893 [04:38<40:32,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█         | 100/893 [04:42<46:18,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█▏        | 101/893 [04:47<48:39,  3.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  11%|█▏        | 102/893 [04:48<41:19,  3.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 103/893 [04:53<46:55,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 104/893 [04:54<34:57,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 105/893 [04:56<32:49,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 106/893 [04:59<35:35,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 107/893 [05:01<32:09,  2.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 108/893 [05:03<30:36,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 109/893 [05:05<30:47,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 110/893 [05:07<29:12,  2.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  12%|█▏        | 111/893 [05:12<39:01,  2.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 112/893 [05:14<35:15,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 113/893 [05:16<31:55,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 114/893 [05:21<42:37,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 115/893 [05:26<47:30,  3.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 116/893 [05:31<53:35,  4.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 117/893 [05:33<45:31,  3.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 118/893 [05:35<38:19,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 119/893 [05:36<34:07,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  13%|█▎        | 120/893 [05:41<41:00,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▎        | 121/893 [05:44<42:13,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▎        | 122/893 [05:46<37:32,  2.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 123/893 [05:48<33:14,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 124/893 [05:50<29:59,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 125/893 [05:52<29:02,  2.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 126/893 [05:54<28:30,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 127/893 [05:59<39:35,  3.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 128/893 [06:03<42:56,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  14%|█▍        | 129/893 [06:05<37:42,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▍        | 130/893 [06:08<34:26,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▍        | 131/893 [06:10<32:58,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▍        | 132/893 [06:12<31:45,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▍        | 133/893 [06:15<34:12,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▌        | 134/893 [06:18<35:30,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▌        | 135/893 [06:22<38:02,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▌        | 136/893 [06:24<35:25,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▌        | 137/893 [06:27<35:03,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  15%|█▌        | 138/893 [06:30<37:42,  3.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 139/893 [06:35<42:47,  3.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 140/893 [06:38<41:39,  3.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 141/893 [06:40<37:23,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 142/893 [06:42<33:43,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 143/893 [06:47<42:04,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 144/893 [06:52<48:59,  3.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▌        | 145/893 [06:57<50:15,  4.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▋        | 146/893 [07:02<55:09,  4.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  16%|█▋        | 147/893 [07:03<43:23,  3.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 148/893 [07:07<43:30,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 149/893 [07:07<31:58,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 150/893 [07:09<30:29,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 151/893 [07:10<22:14,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 152/893 [07:10<16:11,  1.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 153/893 [07:10<13:12,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 154/893 [07:11<10:32,  1.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 155/893 [07:14<21:09,  1.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  17%|█▋        | 156/893 [07:15<15:58,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 157/893 [07:15<11:55,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 158/893 [07:17<17:33,  1.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 159/893 [07:19<19:15,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 160/893 [07:22<24:00,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 161/893 [07:26<28:51,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 162/893 [07:26<20:59,  1.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 163/893 [07:28<22:43,  1.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 164/893 [07:30<23:25,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  18%|█▊        | 165/893 [07:35<34:23,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▊        | 166/893 [07:37<30:21,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▊        | 167/893 [07:39<27:50,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 168/893 [07:44<38:16,  3.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 169/893 [07:44<27:29,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 170/893 [07:46<26:31,  2.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 171/893 [07:48<25:19,  2.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 172/893 [07:50<24:03,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 173/893 [07:53<29:31,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  19%|█▉        | 174/893 [07:56<31:06,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|█▉        | 175/893 [07:59<32:53,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|█▉        | 176/893 [08:02<32:18,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|█▉        | 177/893 [08:04<31:33,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|█▉        | 178/893 [08:07<30:10,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|██        | 179/893 [08:09<31:12,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|██        | 180/893 [08:12<30:58,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|██        | 181/893 [08:14<30:06,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|██        | 182/893 [08:19<38:28,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  20%|██        | 183/893 [08:25<45:57,  3.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 184/893 [08:26<38:18,  3.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 185/893 [08:28<32:39,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 186/893 [08:30<30:09,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 187/893 [08:32<28:57,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 188/893 [08:34<26:28,  2.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██        | 189/893 [08:37<27:23,  2.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██▏       | 190/893 [08:42<38:08,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  21%|██▏       | 191/893 [08:47<44:07,  3.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 192/893 [08:50<39:55,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 193/893 [08:51<34:48,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 194/893 [08:53<30:23,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 195/893 [08:55<27:14,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 196/893 [08:58<29:35,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 197/893 [09:01<30:07,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 198/893 [09:03<27:21,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 199/893 [09:08<37:19,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  22%|██▏       | 200/893 [09:10<32:43,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 201/893 [09:11<29:12,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 202/893 [09:13<27:07,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 203/893 [09:19<36:46,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 204/893 [09:23<42:29,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 205/893 [09:27<43:22,  3.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 206/893 [09:33<48:14,  4.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 207/893 [09:38<51:33,  4.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 208/893 [09:42<51:48,  4.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  23%|██▎       | 209/893 [09:48<54:31,  4.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▎       | 210/893 [09:53<55:54,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▎       | 211/893 [09:58<57:05,  5.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▎       | 212/893 [10:00<46:01,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 213/893 [10:02<39:38,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 214/893 [10:05<36:17,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 215/893 [10:08<34:57,  3.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 216/893 [10:10<31:43,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 217/893 [10:12<31:03,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  24%|██▍       | 218/893 [10:15<30:33,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▍       | 219/893 [10:17<26:47,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▍       | 220/893 [10:19<27:41,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▍       | 221/893 [10:22<27:40,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▍       | 222/893 [10:24<26:39,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▍       | 223/893 [10:26<25:54,  2.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▌       | 224/893 [10:29<26:17,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▌       | 225/893 [10:31<26:23,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▌       | 226/893 [10:36<34:44,  3.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  25%|██▌       | 227/893 [10:41<42:12,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 228/893 [10:43<36:41,  3.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 229/893 [10:45<31:18,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 230/893 [10:47<29:30,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 231/893 [10:49<26:43,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 232/893 [10:51<24:51,  2.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 233/893 [10:56<34:23,  3.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▌       | 234/893 [11:02<41:05,  3.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▋       | 235/893 [11:07<46:25,  4.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  26%|██▋       | 236/893 [11:10<41:07,  3.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 237/893 [11:12<37:14,  3.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 238/893 [11:17<43:13,  3.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 239/893 [11:23<47:26,  4.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 240/893 [11:28<50:13,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 241/893 [11:33<52:31,  4.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 242/893 [11:36<45:15,  4.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 243/893 [11:38<37:12,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 244/893 [11:39<31:41,  2.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  27%|██▋       | 245/893 [11:41<28:10,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 246/893 [11:43<26:14,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 247/893 [11:48<35:16,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 248/893 [11:54<41:45,  3.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 249/893 [11:59<45:09,  4.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 250/893 [12:01<38:02,  3.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 251/893 [12:03<33:11,  3.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 252/893 [12:05<30:14,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 253/893 [12:08<30:04,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  28%|██▊       | 254/893 [12:10<26:59,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▊       | 255/893 [12:12<26:31,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▊       | 256/893 [12:15<27:08,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 257/893 [12:17<27:04,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 258/893 [12:23<35:50,  3.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 259/893 [12:28<41:49,  3.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 260/893 [12:30<36:04,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 261/893 [12:32<31:44,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 262/893 [12:34<29:27,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  29%|██▉       | 263/893 [12:37<28:58,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|██▉       | 264/893 [12:42<37:00,  3.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|██▉       | 265/893 [12:47<41:42,  3.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|██▉       | 266/893 [12:49<35:24,  3.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|██▉       | 267/893 [12:55<41:23,  3.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|███       | 268/893 [12:59<42:12,  4.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|███       | 269/893 [13:04<45:38,  4.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|███       | 270/893 [13:07<39:47,  3.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|███       | 271/893 [13:07<28:18,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  30%|███       | 272/893 [13:12<35:55,  3.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 273/893 [13:17<41:26,  4.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 274/893 [13:21<40:34,  3.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 275/893 [13:24<37:40,  3.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 276/893 [13:29<41:30,  4.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 277/893 [13:32<38:39,  3.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 278/893 [13:36<37:42,  3.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███       | 279/893 [13:40<41:07,  4.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███▏      | 280/893 [13:46<45:22,  4.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  31%|███▏      | 281/893 [13:51<47:37,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 282/893 [13:54<41:29,  4.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 283/893 [13:58<40:40,  4.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 284/893 [14:01<38:46,  3.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 285/893 [14:01<27:35,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 286/893 [14:05<29:32,  2.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 287/893 [14:09<33:21,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 288/893 [14:11<29:54,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 289/893 [14:16<36:45,  3.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  32%|███▏      | 290/893 [14:18<31:20,  3.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 291/893 [14:20<28:00,  2.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 292/893 [14:21<21:04,  2.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 293/893 [14:21<16:14,  1.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 294/893 [14:26<27:04,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 295/893 [14:32<34:24,  3.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 296/893 [14:32<24:39,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 297/893 [14:36<30:46,  3.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 298/893 [14:42<37:15,  3.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  33%|███▎      | 299/893 [14:47<41:43,  4.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▎      | 300/893 [14:49<35:57,  3.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▎      | 301/893 [14:54<40:36,  4.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 302/893 [15:00<44:01,  4.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 303/893 [15:05<46:27,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 304/893 [15:08<41:30,  4.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 305/893 [15:10<34:50,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 306/893 [15:12<30:53,  3.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 307/893 [15:18<37:07,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  34%|███▍      | 308/893 [15:20<32:55,  3.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▍      | 309/893 [15:23<30:31,  3.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▍      | 310/893 [15:25<27:20,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▍      | 311/893 [15:27<25:24,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▍      | 312/893 [15:29<23:29,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▌      | 313/893 [15:31<22:09,  2.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▌      | 314/893 [15:36<30:51,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▌      | 315/893 [15:41<36:24,  3.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▌      | 316/893 [15:47<41:02,  4.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  35%|███▌      | 317/893 [15:48<33:10,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 318/893 [15:50<29:50,  3.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 319/893 [15:56<35:25,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 320/893 [16:01<39:47,  4.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 321/893 [16:04<37:27,  3.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 322/893 [16:08<36:56,  3.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▌      | 323/893 [16:11<33:19,  3.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▋      | 324/893 [16:14<32:10,  3.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  36%|███▋      | 325/893 [16:19<37:22,  3.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 326/893 [16:19<26:47,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 327/893 [16:23<28:17,  3.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 328/893 [16:25<26:49,  2.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 329/893 [16:30<33:42,  3.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 330/893 [16:36<38:31,  4.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 331/893 [16:38<33:29,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 332/893 [16:41<30:54,  3.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 333/893 [16:43<28:05,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  37%|███▋      | 334/893 [16:45<26:03,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 335/893 [16:50<31:41,  3.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 336/893 [16:54<33:28,  3.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 337/893 [17:00<38:22,  4.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 338/893 [17:05<41:33,  4.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 339/893 [17:07<35:04,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 340/893 [17:10<31:50,  3.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 341/893 [17:12<28:00,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 342/893 [17:14<25:04,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  38%|███▊      | 343/893 [17:17<24:54,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▊      | 344/893 [17:19<25:15,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▊      | 345/893 [17:25<32:33,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▊      | 346/893 [17:30<36:58,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 347/893 [17:33<33:29,  3.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 348/893 [17:38<37:31,  4.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 349/893 [17:43<40:14,  4.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 350/893 [17:46<34:34,  3.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 351/893 [17:47<29:01,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  39%|███▉      | 352/893 [17:50<27:30,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|███▉      | 353/893 [17:53<27:12,  3.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|███▉      | 354/893 [17:55<25:28,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|███▉      | 355/893 [17:57<23:20,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|███▉      | 356/893 [17:58<16:55,  1.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|███▉      | 357/893 [18:00<19:15,  2.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|████      | 358/893 [18:03<20:53,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|████      | 359/893 [18:05<19:37,  2.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|████      | 360/893 [18:07<20:04,  2.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  40%|████      | 361/893 [18:10<20:22,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 362/893 [18:12<20:23,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 363/893 [18:17<26:59,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 364/893 [18:22<32:14,  3.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 365/893 [18:27<36:16,  4.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 366/893 [18:29<30:28,  3.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 367/893 [18:31<26:32,  3.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████      | 368/893 [18:33<24:18,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████▏     | 369/893 [18:34<17:30,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  41%|████▏     | 370/893 [18:34<12:46,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 371/893 [18:39<23:09,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 372/893 [18:42<23:31,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 373/893 [18:44<21:21,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 374/893 [18:47<21:58,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 375/893 [18:49<21:36,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 376/893 [18:54<28:38,  3.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 377/893 [19:00<33:40,  3.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 378/893 [19:04<35:47,  4.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  42%|████▏     | 379/893 [19:07<32:09,  3.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 380/893 [19:10<28:55,  3.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 381/893 [19:15<33:34,  3.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 382/893 [19:20<37:03,  4.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 383/893 [19:26<39:18,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 384/893 [19:27<32:18,  3.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 385/893 [19:29<27:27,  3.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 386/893 [19:31<24:28,  2.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 387/893 [19:33<22:19,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  43%|████▎     | 388/893 [19:34<17:58,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▎     | 389/893 [19:36<16:23,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▎     | 390/893 [19:38<17:28,  2.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 391/893 [19:40<16:42,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 392/893 [19:40<12:16,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 393/893 [19:42<13:47,  1.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 394/893 [19:48<22:33,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 395/893 [19:53<28:39,  3.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 396/893 [19:58<33:02,  3.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  44%|████▍     | 397/893 [20:00<27:17,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▍     | 398/893 [20:03<25:56,  3.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▍     | 399/893 [20:05<23:01,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▍     | 400/893 [20:10<29:25,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▍     | 401/893 [20:13<26:55,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▌     | 402/893 [20:15<24:22,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▌     | 403/893 [20:17<21:47,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▌     | 404/893 [20:19<19:40,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▌     | 405/893 [20:20<18:06,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  45%|████▌     | 406/893 [20:26<25:34,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 407/893 [20:31<30:38,  3.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 408/893 [20:33<26:34,  3.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 409/893 [20:36<25:37,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 410/893 [20:38<22:50,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 411/893 [20:41<22:22,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 412/893 [20:43<20:39,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▌     | 413/893 [20:46<21:26,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▋     | 414/893 [20:49<21:48,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  46%|████▋     | 415/893 [20:51<20:50,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 416/893 [20:53<18:53,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 417/893 [20:55<18:15,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 418/893 [21:00<25:09,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 419/893 [21:03<24:16,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 420/893 [21:05<22:11,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 421/893 [21:07<20:12,  2.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 422/893 [21:07<14:46,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 423/893 [21:10<15:37,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  47%|████▋     | 424/893 [21:12<17:01,  2.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 425/893 [21:14<16:52,  2.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 426/893 [21:20<24:15,  3.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 427/893 [21:22<22:49,  2.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 428/893 [21:24<21:13,  2.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 429/893 [21:27<19:40,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 430/893 [21:30<20:30,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 431/893 [21:33<21:59,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 432/893 [21:35<20:23,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  48%|████▊     | 433/893 [21:38<20:21,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▊     | 434/893 [21:41<21:33,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▊     | 435/893 [21:43<19:33,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 436/893 [21:45<18:55,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 437/893 [21:48<18:59,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 438/893 [21:50<17:36,  2.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 439/893 [21:52<16:53,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 440/893 [21:54<16:38,  2.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 441/893 [21:55<15:30,  2.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  49%|████▉     | 442/893 [21:57<14:50,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|████▉     | 443/893 [22:00<16:40,  2.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|████▉     | 444/893 [22:02<16:11,  2.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|████▉     | 445/893 [22:05<16:51,  2.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|████▉     | 446/893 [22:07<17:00,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|█████     | 447/893 [22:12<23:39,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|█████     | 448/893 [22:14<21:01,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|█████     | 449/893 [22:16<19:48,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  50%|█████     | 450/893 [22:19<18:46,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 451/893 [22:20<16:10,  2.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 452/893 [22:22<15:16,  2.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 453/893 [22:24<15:18,  2.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 454/893 [22:26<15:40,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 455/893 [22:28<15:38,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 456/893 [22:31<16:13,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████     | 457/893 [22:33<15:49,  2.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████▏    | 458/893 [22:35<16:08,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  51%|█████▏    | 459/893 [22:37<15:34,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 460/893 [22:40<16:57,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 461/893 [22:42<16:03,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 462/893 [22:44<15:25,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 463/893 [22:47<17:14,  2.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 464/893 [22:49<17:03,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 465/893 [22:52<16:40,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 466/893 [22:54<16:13,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 467/893 [22:59<22:28,  3.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  52%|█████▏    | 468/893 [23:02<23:17,  3.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 469/893 [23:06<23:51,  3.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 470/893 [23:10<25:16,  3.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 471/893 [23:10<18:13,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 472/893 [23:16<23:38,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 473/893 [23:18<21:43,  3.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 474/893 [23:23<26:19,  3.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 475/893 [23:26<23:29,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 476/893 [23:29<22:22,  3.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  53%|█████▎    | 477/893 [23:31<20:38,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▎    | 478/893 [23:33<17:38,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▎    | 479/893 [23:35<17:57,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 480/893 [23:38<18:49,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 481/893 [23:43<23:30,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 482/893 [23:46<21:02,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 483/893 [23:48<20:19,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 484/893 [23:51<18:27,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 485/893 [23:54<19:38,  2.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  54%|█████▍    | 486/893 [23:57<19:25,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▍    | 487/893 [23:59<17:59,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▍    | 488/893 [24:02<18:01,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▍    | 489/893 [24:04<16:44,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▍    | 490/893 [24:09<22:18,  3.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▍    | 491/893 [24:10<17:31,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▌    | 492/893 [24:12<16:31,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▌    | 493/893 [24:13<13:23,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▌    | 494/893 [24:16<14:57,  2.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  55%|█████▌    | 495/893 [24:21<21:14,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 496/893 [24:26<25:18,  3.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 497/893 [24:29<22:51,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 498/893 [24:31<20:44,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 499/893 [24:37<24:38,  3.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 500/893 [24:39<21:28,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 501/893 [24:41<18:54,  2.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▌    | 502/893 [24:46<23:21,  3.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▋    | 503/893 [24:51<26:33,  4.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  56%|█████▋    | 504/893 [24:53<22:41,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 505/893 [24:59<25:52,  4.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 506/893 [25:01<23:23,  3.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 507/893 [25:04<21:22,  3.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 508/893 [25:06<19:37,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 509/893 [25:12<23:39,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 510/893 [25:14<21:04,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 511/893 [25:16<18:53,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 512/893 [25:18<17:15,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  57%|█████▋    | 513/893 [25:19<13:07,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 514/893 [25:24<19:20,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 515/893 [25:29<23:17,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 516/893 [25:31<18:34,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 517/893 [25:36<22:52,  3.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 518/893 [25:38<20:54,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 519/893 [25:41<19:37,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 520/893 [25:44<19:01,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 521/893 [25:48<20:20,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  58%|█████▊    | 522/893 [25:51<19:51,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▊    | 523/893 [25:56<22:35,  3.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▊    | 524/893 [25:56<16:24,  2.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 525/893 [25:58<15:51,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 526/893 [25:59<12:28,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 527/893 [26:01<12:34,  2.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 528/893 [26:04<13:06,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 529/893 [26:06<14:26,  2.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 530/893 [26:12<19:50,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  59%|█████▉    | 531/893 [26:14<17:56,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|█████▉    | 532/893 [26:16<16:20,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|█████▉    | 533/893 [26:17<12:07,  2.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|█████▉    | 534/893 [26:20<15:07,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|█████▉    | 535/893 [26:22<13:30,  2.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|██████    | 536/893 [26:27<18:45,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|██████    | 537/893 [26:31<18:59,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|██████    | 538/893 [26:33<17:56,  3.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|██████    | 539/893 [26:34<13:08,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  60%|██████    | 540/893 [26:39<18:29,  3.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 541/893 [26:44<21:58,  3.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 542/893 [26:47<21:09,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 543/893 [26:50<19:38,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 544/893 [26:54<20:50,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 545/893 [27:00<23:53,  4.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████    | 546/893 [27:05<26:12,  4.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████▏   | 547/893 [27:10<27:36,  4.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████▏   | 548/893 [27:13<23:24,  4.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  61%|██████▏   | 549/893 [27:15<20:31,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 550/893 [27:16<15:25,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 551/893 [27:16<11:03,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 552/893 [27:19<13:18,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 553/893 [27:23<16:19,  2.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 554/893 [27:27<17:07,  3.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 555/893 [27:29<15:49,  2.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 556/893 [27:29<11:27,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 557/893 [27:32<12:50,  2.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  62%|██████▏   | 558/893 [27:35<13:33,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 559/893 [27:38<13:42,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 560/893 [27:40<13:39,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 561/893 [27:44<15:30,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 562/893 [27:47<16:10,  2.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 563/893 [27:52<20:01,  3.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 564/893 [27:52<14:25,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 565/893 [27:53<10:31,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 566/893 [27:57<13:42,  2.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  63%|██████▎   | 567/893 [27:59<14:04,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▎   | 568/893 [28:04<17:38,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▎   | 569/893 [28:05<12:54,  2.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 570/893 [28:07<13:39,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 571/893 [28:10<13:25,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 572/893 [28:12<13:02,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 573/893 [28:15<13:07,  2.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 574/893 [28:17<12:37,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  64%|██████▍   | 575/893 [28:19<11:44,  2.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▍   | 576/893 [28:21<11:14,  2.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▍   | 577/893 [28:22<10:53,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▍   | 578/893 [28:25<11:34,  2.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▍   | 579/893 [28:28<12:14,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▍   | 580/893 [28:31<13:01,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▌   | 581/893 [28:33<12:39,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▌   | 582/893 [28:35<12:24,  2.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▌   | 583/893 [28:38<13:03,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  65%|██████▌   | 584/893 [28:42<15:34,  3.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 585/893 [28:46<16:47,  3.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 586/893 [28:50<18:30,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 587/893 [28:55<19:24,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 588/893 [28:57<17:46,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 589/893 [29:01<17:20,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 590/893 [29:03<16:11,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▌   | 591/893 [29:07<17:28,  3.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▋   | 592/893 [29:11<17:20,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  66%|██████▋   | 593/893 [29:12<13:38,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 594/893 [29:14<13:10,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 595/893 [29:18<15:06,  3.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 596/893 [29:21<14:04,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 597/893 [29:26<17:28,  3.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 598/893 [29:31<19:51,  4.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 599/893 [29:31<14:12,  2.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 600/893 [29:34<13:33,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 601/893 [29:36<12:42,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  67%|██████▋   | 602/893 [29:38<11:58,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 603/893 [29:40<10:41,  2.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 604/893 [29:40<07:49,  1.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 605/893 [29:42<08:14,  1.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 606/893 [29:45<09:53,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 607/893 [29:47<10:21,  2.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 608/893 [29:48<07:39,  1.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 609/893 [29:49<07:58,  1.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 610/893 [29:53<10:28,  2.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  68%|██████▊   | 611/893 [29:57<13:41,  2.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▊   | 612/893 [30:00<13:23,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▊   | 613/893 [30:02<11:44,  2.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 614/893 [30:07<15:33,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 615/893 [30:10<14:40,  3.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 616/893 [30:15<17:32,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 617/893 [30:20<19:19,  4.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 618/893 [30:26<20:38,  4.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 619/893 [30:26<14:49,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  69%|██████▉   | 620/893 [30:28<13:53,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|██████▉   | 621/893 [30:31<13:06,  2.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|██████▉   | 622/893 [30:36<16:19,  3.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|██████▉   | 623/893 [30:41<18:23,  4.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|██████▉   | 624/893 [30:44<16:40,  3.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|██████▉   | 625/893 [30:47<15:20,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|███████   | 626/893 [30:50<14:19,  3.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|███████   | 627/893 [30:52<12:22,  2.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|███████   | 628/893 [30:52<09:07,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  70%|███████   | 629/893 [30:54<09:25,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 630/893 [30:57<09:55,  2.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 631/893 [31:02<13:52,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 632/893 [31:05<13:04,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 633/893 [31:08<13:45,  3.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 634/893 [31:11<13:14,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 635/893 [31:16<15:57,  3.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████   | 636/893 [31:18<13:49,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████▏  | 637/893 [31:24<16:15,  3.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  71%|███████▏  | 638/893 [31:26<13:51,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 639/893 [31:29<14:23,  3.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 640/893 [31:32<13:15,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 641/893 [31:35<12:41,  3.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 642/893 [31:40<15:32,  3.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 643/893 [31:45<17:32,  4.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 644/893 [31:47<14:50,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 645/893 [31:53<16:38,  4.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 646/893 [31:56<15:37,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  72%|███████▏  | 647/893 [31:58<14:05,  3.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 648/893 [32:01<13:20,  3.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 649/893 [32:05<13:23,  3.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 650/893 [32:08<13:30,  3.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 651/893 [32:11<13:01,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 652/893 [32:14<12:45,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 653/893 [32:17<12:35,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 654/893 [32:20<12:00,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 655/893 [32:25<14:46,  3.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  73%|███████▎  | 656/893 [32:31<16:46,  4.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▎  | 657/893 [32:36<17:56,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▎  | 658/893 [32:41<18:48,  4.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 659/893 [32:44<16:10,  4.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 660/893 [32:46<13:30,  3.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 661/893 [32:46<09:52,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 662/893 [32:52<13:00,  3.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 663/893 [32:52<09:30,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 664/893 [32:57<12:12,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  74%|███████▍  | 665/893 [32:59<11:24,  3.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▍  | 666/893 [33:02<10:48,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▍  | 667/893 [33:05<10:48,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▍  | 668/893 [33:10<13:31,  3.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▍  | 669/893 [33:15<15:22,  4.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▌  | 670/893 [33:18<13:17,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▌  | 671/893 [33:23<15:11,  4.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▌  | 672/893 [33:26<13:34,  3.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▌  | 673/893 [33:28<12:13,  3.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  75%|███████▌  | 674/893 [33:31<11:39,  3.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 675/893 [33:34<11:11,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 676/893 [33:34<08:09,  2.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 677/893 [33:37<08:30,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 678/893 [33:39<08:37,  2.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 679/893 [33:45<11:33,  3.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▌  | 680/893 [33:50<13:36,  3.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▋  | 681/893 [33:52<12:05,  3.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▋  | 682/893 [33:56<12:08,  3.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  76%|███████▋  | 683/893 [33:59<12:07,  3.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 684/893 [34:02<10:55,  3.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 685/893 [34:04<10:24,  3.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 686/893 [34:10<12:35,  3.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 687/893 [34:15<14:06,  4.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 688/893 [34:18<13:27,  3.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 689/893 [34:20<11:20,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 690/893 [34:23<10:19,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 691/893 [34:26<10:07,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  77%|███████▋  | 692/893 [34:28<09:35,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 693/893 [34:28<06:50,  2.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 694/893 [34:31<07:04,  2.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 695/893 [34:33<07:31,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 696/893 [34:36<07:54,  2.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 697/893 [34:38<07:15,  2.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 698/893 [34:40<07:33,  2.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 699/893 [34:41<05:50,  1.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 700/893 [34:41<04:18,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  78%|███████▊  | 701/893 [34:46<08:05,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▊  | 702/893 [34:52<10:38,  3.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▊  | 703/893 [34:57<12:22,  3.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 704/893 [35:02<13:31,  4.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 705/893 [35:07<14:16,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 706/893 [35:12<14:48,  4.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 707/893 [35:14<12:03,  3.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 708/893 [35:17<10:41,  3.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  79%|███████▉  | 709/893 [35:19<09:45,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|███████▉  | 710/893 [35:20<07:02,  2.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|███████▉  | 711/893 [35:23<08:04,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|███████▉  | 712/893 [35:26<08:22,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|███████▉  | 713/893 [35:30<08:55,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|███████▉  | 714/893 [35:32<08:44,  2.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|████████  | 715/893 [35:35<08:01,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|████████  | 716/893 [35:37<07:27,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|████████  | 717/893 [35:40<07:46,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  80%|████████  | 718/893 [35:43<08:06,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 719/893 [35:45<07:56,  2.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 720/893 [35:49<08:34,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 721/893 [35:53<09:49,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 722/893 [35:57<10:01,  3.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 723/893 [35:58<08:09,  2.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 724/893 [36:00<06:39,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████  | 725/893 [36:04<08:26,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████▏ | 726/893 [36:08<09:10,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  81%|████████▏ | 727/893 [36:12<09:19,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 728/893 [36:13<07:50,  2.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 729/893 [36:17<08:09,  2.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 730/893 [36:19<07:22,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 731/893 [36:23<08:47,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 732/893 [36:25<07:16,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 733/893 [36:28<07:26,  2.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 734/893 [36:30<07:21,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 735/893 [36:33<07:15,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  82%|████████▏ | 736/893 [36:38<08:45,  3.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 737/893 [36:41<08:20,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 738/893 [36:43<07:21,  2.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 739/893 [36:45<06:34,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 740/893 [36:47<06:27,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 741/893 [36:52<08:25,  3.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 742/893 [36:54<07:35,  3.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 743/893 [36:56<06:47,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 744/893 [36:58<06:10,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  83%|████████▎ | 745/893 [37:00<05:39,  2.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▎ | 746/893 [37:04<06:24,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▎ | 747/893 [37:04<04:51,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 748/893 [37:08<06:22,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 749/893 [37:09<04:43,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 750/893 [37:13<06:28,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 751/893 [37:17<06:57,  2.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 752/893 [37:21<07:55,  3.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 753/893 [37:25<08:00,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  84%|████████▍ | 754/893 [37:29<08:26,  3.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▍ | 755/893 [37:29<06:14,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▍ | 756/893 [37:33<06:43,  2.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▍ | 757/893 [37:35<05:58,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▍ | 758/893 [37:39<06:57,  3.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▍ | 759/893 [37:41<06:10,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▌ | 760/893 [37:45<06:45,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▌ | 761/893 [37:45<05:14,  2.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▌ | 762/893 [37:48<05:34,  2.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  85%|████████▌ | 763/893 [37:53<06:37,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 764/893 [37:54<05:23,  2.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 765/893 [37:55<04:31,  2.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 766/893 [37:57<04:12,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 767/893 [38:01<05:49,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 768/893 [38:04<05:39,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 769/893 [38:06<04:59,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▌ | 770/893 [38:09<05:23,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▋ | 771/893 [38:11<05:04,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  86%|████████▋ | 772/893 [38:14<05:34,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 773/893 [38:15<04:04,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 774/893 [38:15<03:15,  1.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 775/893 [38:20<05:17,  2.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 776/893 [38:22<04:42,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 777/893 [38:27<05:55,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 778/893 [38:32<07:05,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 779/893 [38:35<06:28,  3.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 780/893 [38:37<05:34,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  87%|████████▋ | 781/893 [38:38<04:42,  2.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 782/893 [38:39<03:58,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 783/893 [38:41<03:26,  1.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 784/893 [38:42<03:02,  1.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 785/893 [38:45<03:41,  2.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 786/893 [38:50<05:16,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 787/893 [38:53<05:32,  3.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 788/893 [38:58<06:07,  3.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 789/893 [39:03<06:45,  3.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  88%|████████▊ | 790/893 [39:07<06:48,  3.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▊ | 791/893 [39:11<06:53,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▊ | 792/893 [39:12<05:26,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 793/893 [39:13<03:57,  2.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 794/893 [39:15<03:40,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 795/893 [39:16<03:12,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 796/893 [39:20<04:13,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 797/893 [39:22<03:39,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 798/893 [39:26<04:50,  3.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  89%|████████▉ | 799/893 [39:31<05:35,  3.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|████████▉ | 800/893 [39:36<06:03,  3.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|████████▉ | 801/893 [39:39<05:33,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|████████▉ | 802/893 [39:43<05:48,  3.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|████████▉ | 803/893 [39:44<04:11,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|█████████ | 804/893 [39:48<04:49,  3.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|█████████ | 805/893 [39:52<05:07,  3.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|█████████ | 806/893 [39:55<04:58,  3.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|█████████ | 807/893 [39:57<04:18,  3.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  90%|█████████ | 808/893 [39:59<03:32,  2.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 809/893 [40:00<02:55,  2.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 810/893 [40:01<02:28,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 811/893 [40:02<02:08,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 812/893 [40:06<03:06,  2.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 813/893 [40:10<03:49,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████ | 814/893 [40:15<04:42,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████▏| 815/893 [40:19<04:49,  3.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████▏| 816/893 [40:24<05:20,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  91%|█████████▏| 817/893 [40:27<04:42,  3.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 818/893 [40:32<05:08,  4.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 819/893 [40:37<05:15,  4.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 820/893 [40:41<05:19,  4.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 821/893 [40:45<04:55,  4.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 822/893 [40:46<03:47,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 823/893 [40:47<03:05,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 824/893 [40:49<02:37,  2.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 825/893 [40:53<03:17,  2.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  92%|█████████▏| 826/893 [40:57<03:29,  3.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 827/893 [40:59<03:03,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 828/893 [41:01<02:47,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 829/893 [41:02<02:22,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 830/893 [41:03<01:58,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 831/893 [41:04<01:37,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 832/893 [41:06<01:45,  1.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 833/893 [41:09<01:58,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  93%|█████████▎| 834/893 [41:11<01:57,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▎| 835/893 [41:13<01:50,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▎| 836/893 [41:17<02:26,  2.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▎| 837/893 [41:20<02:33,  2.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 838/893 [41:22<02:16,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 839/893 [41:23<01:56,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 840/893 [41:25<01:43,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 841/893 [41:27<01:50,  2.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 842/893 [41:33<02:38,  3.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  94%|█████████▍| 843/893 [41:37<02:58,  3.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▍| 844/893 [41:41<03:01,  3.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▍| 845/893 [41:43<02:25,  3.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▍| 846/893 [41:44<02:04,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▍| 847/893 [41:49<02:34,  3.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▍| 848/893 [41:54<02:51,  3.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▌| 849/893 [41:57<02:29,  3.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▌| 850/893 [41:58<01:59,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▌| 851/893 [42:03<02:24,  3.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  95%|█████████▌| 852/893 [42:08<02:43,  3.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 853/893 [42:10<02:10,  3.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 854/893 [42:12<01:50,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 855/893 [42:13<01:28,  2.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 856/893 [42:18<01:58,  3.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 857/893 [42:23<02:10,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 858/893 [42:24<01:43,  2.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▌| 859/893 [42:26<01:24,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▋| 860/893 [42:27<01:16,  2.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  96%|█████████▋| 861/893 [42:29<01:02,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 862/893 [42:34<01:30,  2.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 863/893 [42:35<01:12,  2.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 864/893 [42:37<01:07,  2.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 865/893 [42:41<01:20,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 866/893 [42:43<01:10,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 867/893 [42:46<01:10,  2.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 868/893 [42:51<01:23,  3.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 869/893 [42:54<01:14,  3.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  97%|█████████▋| 870/893 [42:55<00:59,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 871/893 [42:56<00:48,  2.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 872/893 [42:59<00:47,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 873/893 [43:02<00:49,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 874/893 [43:04<00:46,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 875/893 [43:07<00:45,  2.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 876/893 [43:08<00:35,  2.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 877/893 [43:10<00:34,  2.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 878/893 [43:11<00:27,  1.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  98%|█████████▊| 879/893 [43:16<00:37,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▊| 880/893 [43:17<00:29,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▊| 881/893 [43:21<00:33,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 882/893 [43:24<00:29,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 883/893 [43:28<00:31,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 884/893 [43:28<00:20,  2.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 885/893 [43:28<00:13,  1.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 886/893 [43:29<00:08,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 887/893 [43:30<00:07,  1.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference:  99%|█████████▉| 888/893 [43:31<00:06,  1.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference: 100%|█████████▉| 889/893 [43:33<00:06,  1.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference: 100%|█████████▉| 890/893 [43:35<00:04,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference: 100%|█████████▉| 891/893 [43:40<00:04,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference: 100%|█████████▉| 892/893 [43:41<00:02,  2.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "LLAMA Inference: 100%|██████████| 893/893 [43:44<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done with llama. Results saved to chatbot_result/zero_shot/scenario_A_zero_shot_results_llama.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "scenario = \"zero_shot\"\n",
    "MODELS = {\n",
    "    \"llama\":    \"meta-llama/Llama-3.2-1B\",\n",
    "    # \"qwen\":     \"Qwen/Qwen3-0.6B-Base\",\n",
    "    # \"olmo\":     \"allenai/OLMo-2-0425-1B\",\n",
    "}\n",
    "\n",
    "OFFLOAD_DIR = \"./offload\"\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "SAVE_EVERY = 20\n",
    "\n",
    "# === Load test set (pre-split externally)\n",
    "questions = test_df[\"instruction\"].cast(str).to_list()\n",
    "answers = test_df[\"response\"].cast(str).to_list()\n",
    "\n",
    "# === Safe append function\n",
    "def append_to_csv(new_data: pl.DataFrame, path: str):\n",
    "    if os.path.exists(path):\n",
    "        existing = pl.read_csv(path)\n",
    "        combined = existing.vstack(new_data)\n",
    "    else:\n",
    "        combined = new_data\n",
    "    combined.write_csv(path)\n",
    "\n",
    "def adaptive_max_new_tokens(gold_text: str, tokenizer,\n",
    "                            min_nt=48, max_nt=200, mult=1.3, bonus=16) -> int:\n",
    "    # estimate needed length from gold answer tokens\n",
    "    n_gold = len(tokenizer(gold_text, add_special_tokens=False).input_ids)\n",
    "    est = int(round(mult * n_gold) + bonus)   # headroom\n",
    "    return int(np.clip(est, min_nt, max_nt))\n",
    "\n",
    "# === Loop through each model\n",
    "for key, MODEL_NAME in MODELS.items():\n",
    "    OUTPUT_CSV = f\"chatbot_result/{scenario}/scenario_A_zero_shot_results_{key}.csv\"\n",
    "\n",
    "    print(f\"\\n🚀 Running inference for: {MODEL_NAME}\")\n",
    "    os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # Load tokenizer + model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=True, use_fast=True, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if USE_CUDA else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=OFFLOAD_DIR\n",
    "    )\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Resume support\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        existing = pl.read_csv(OUTPUT_CSV)\n",
    "        done = set(existing[\"instruction\"].to_list())\n",
    "        print(f\"🔄 Resuming from {len(done)} completed rows...\")\n",
    "    else:\n",
    "        done = set()\n",
    "\n",
    "    results = []\n",
    "    for question, ground_truth in tqdm(zip(questions, answers), total=len(questions), desc=f\"{key.upper()} Inference\"):\n",
    "        if question in done:\n",
    "            continue\n",
    "\n",
    "        prompt = f\"\"\"You are a helpful retail assistant. Answer the following customer query briefly and accurately.\\n\\nCustomer: {question}\\nAnswer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            max_nt = adaptive_max_new_tokens(ground_truth, tokenizer,\n",
    "                                            min_nt=48, max_nt=300, mult=1.3, bonus=16)\n",
    "\n",
    "            output = pipe(prompt, max_new_tokens=max_nt, do_sample=False)[0][\"generated_text\"]\n",
    "            if \"Answer:\" in output:\n",
    "                answer_part = output.split(\"Answer:\")[1]\n",
    "                predicted = answer_part.split(\"Customer:\")[0].strip() if \"Customer:\" in answer_part else answer_part.strip()\n",
    "            else:\n",
    "                predicted = output.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing question: {question[:60]}... Skipping. Reason: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append((question, ground_truth, predicted))\n",
    "\n",
    "        if len(results) >= SAVE_EVERY:\n",
    "            # Convert batch of results to DataFrame\n",
    "            chunk_df = pl.DataFrame(results, schema=[\"instruction\", \"response\", \"prediction\"])\n",
    "            # Join with full test_df to get original columns\n",
    "            enriched = test_df.join(chunk_df, on=[\"instruction\", \"response\"], how=\"inner\")\n",
    "            append_to_csv(enriched, OUTPUT_CSV)\n",
    "            results = []\n",
    "\n",
    "    if results:\n",
    "        chunk_df = pl.DataFrame(results, schema=[\"instruction\", \"response\", \"prediction\"])\n",
    "        enriched = test_df.join(chunk_df, on=[\"instruction\", \"response\"], how=\"inner\")\n",
    "        append_to_csv(enriched, OUTPUT_CSV)\n",
    "\n",
    "    print(f\"✅ Done with {key}. Results saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da123240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Running Evaluation for: meta-llama/Llama-3.2-1B\n",
      "\n",
      "🚀 Running Evaluation for: Qwen/Qwen3-0.6B-Base\n",
      "\n",
      "🚀 Running Evaluation for: allenai/OLMo-2-0425-1B\n",
      "\n",
      "📊 Combined (pivot) comparison:\n",
      "shape: (5, 4)\n",
      "┌──────────────┬───────┬───────┬───────┐\n",
      "│ Metric       ┆ llama ┆ qwen  ┆ olmo  │\n",
      "│ ---          ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ str          ┆ f64   ┆ f64   ┆ f64   │\n",
      "╞══════════════╪═══════╪═══════╪═══════╡\n",
      "│ F1-overlap   ┆ 14.01 ┆ 21.6  ┆ 14.04 │\n",
      "│ BLEU         ┆ 1.7   ┆ 3.36  ┆ 1.15  │\n",
      "│ ROUGE-L      ┆ 11.85 ┆ 16.39 ┆ 11.39 │\n",
      "│ METEOR       ┆ 6.92  ┆ 10.77 ┆ 6.52  │\n",
      "│ BERTScore-F1 ┆ 8.67  ┆ 20.91 ┆ 11.25 │\n",
      "└──────────────┴───────┴───────┴───────┘\n",
      "\n",
      "✅ Saved under 'Evaluation/'\n",
      "- Scenario_A_Evaluation_AllModels.csv\n",
      "- Scenario_A_Evaluation_Pivot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7254/1253071841.py:133: DeprecationWarning: the argument `columns` for `DataFrame.pivot` is deprecated. It was renamed to `on` in version 1.0.0.\n",
      "  pivot_df = final_df.pivot(index=\"Metric\", columns=\"Model\", values=\"Score\")\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "scenario = \"zero_shot\"\n",
    "DEVICE = \"cuda\"\n",
    "MODELS = {\n",
    "    \"llama\":    \"meta-llama/Llama-3.2-1B\",\n",
    "    \"qwen\":     \"Qwen/Qwen3-0.6B-Base\",\n",
    "    \"olmo\":     \"allenai/OLMo-2-0425-1B\",\n",
    "}\n",
    "\n",
    "BERTSCORE_MODEL = \"microsoft/deberta-base-mnli\"  # fast & solid for FAQ/QA\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = \"Evaluation\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# BLEU smoothing (method4 is friendlier for short answers)\n",
    "smooth_fn = SmoothingFunction().method4\n",
    "# Cache rouge scorer once\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# === Helpers ===\n",
    "def _safe_text(x):\n",
    "    return \"\" if x is None or (isinstance(x, float) and math.isnan(x)) else str(x)\n",
    "\n",
    "def normalize(text: str) -> list[str]:\n",
    "    text = _safe_text(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "# def exact_match(pred: str, gold: str) -> int:\n",
    "#     return int(_safe_text(pred).strip().lower() == _safe_text(gold).strip().lower())\n",
    "\n",
    "def f1_score_overlap(pred: str, gold: str) -> float:\n",
    "    pred_tokens = normalize(pred)\n",
    "    gold_tokens = normalize(gold)\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def bleu_score(pred: str, gold: str) -> float:\n",
    "    # BLEU-2 is fairer for short FAQ answers\n",
    "    return sentence_bleu(\n",
    "        [normalize(gold)], normalize(pred),\n",
    "        weights=(0.5, 0.5, 0, 0),\n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "\n",
    "def rouge_l_score(pred: str, gold: str) -> float:\n",
    "    return rouge.score(_safe_text(gold), _safe_text(pred))[\"rougeL\"].fmeasure\n",
    "\n",
    "def meteor(pred: str, gold: str) -> float:\n",
    "    return meteor_score([normalize(gold)], normalize(pred))\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = _safe_text(t).strip()\n",
    "    for pre in (\"answer:\", \"response:\", \"reply:\"):\n",
    "        if t.lower().startswith(pre):\n",
    "            t = t[len(pre):].strip()\n",
    "    return t.strip(\"`\").strip()\n",
    "\n",
    "def compute_metrics(preds_raw, golds_raw):\n",
    "    preds = [clean_text(p) for p in preds_raw]\n",
    "    golds = [clean_text(g) for g in golds_raw]\n",
    "\n",
    "    # em_list, f1_list, bleu_list, rouge_list, meteor_list = [], [], [], [], []\n",
    "    f1_list, bleu_list, rouge_list, meteor_list = [], [], [], []\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        # em_list.append(exact_match(pred, gold))\n",
    "        f1_list.append(f1_score_overlap(pred, gold))\n",
    "        bleu_list.append(bleu_score(pred, gold))\n",
    "        rouge_list.append(rouge_l_score(pred, gold))\n",
    "        meteor_list.append(meteor(pred, gold))\n",
    "\n",
    "    # BERTScore on CUDA; drop empty pairs to avoid warnings\n",
    "    mask = [bool(p.strip()) and bool(g.strip()) for p, g in zip(preds, golds)]\n",
    "    preds_nz = [p for p, m in zip(preds, mask) if m]\n",
    "    golds_nz = [g for g, m in zip(golds, mask) if m]\n",
    "    if len(preds_nz) == 0:\n",
    "        bert_f1_avg = 0.0\n",
    "    else:\n",
    "        P, R, F1 = bertscore(\n",
    "            preds_nz, golds_nz,\n",
    "            lang=\"en\",\n",
    "            model_type=BERTSCORE_MODEL,\n",
    "            rescale_with_baseline=True,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        bert_f1_avg = float(F1.mean()) * 100.0\n",
    "\n",
    "    n = max(1, len(f1_list))\n",
    "    metrics = [\n",
    "        # \"Exact Match\", \n",
    "        \"F1-overlap\", \"BLEU\", \"ROUGE-L\", \"METEOR\", \"BERTScore-F1\"]\n",
    "    scores = [\n",
    "        # round(sum(em_list) / n * 100, 2),\n",
    "        round(sum(f1_list) / n * 100, 2),\n",
    "        round(sum(bleu_list) / n * 100, 2),\n",
    "        round(sum(rouge_list) / n * 100, 2),\n",
    "        round(sum(meteor_list) / n * 100, 2),\n",
    "        round(bert_f1_avg, 2),\n",
    "    ]\n",
    "    assert len(metrics) == len(scores), f\"metrics={len(metrics)} scores={len(scores)}\"\n",
    "\n",
    "    rows = [{\"Metric\": m, \"Score\": s} for m, s in zip(metrics, scores)]\n",
    "    return pl.DataFrame(rows)\n",
    "\n",
    "# === Run all models and combine outputs ===\n",
    "all_summaries = []\n",
    "\n",
    "for key, MODEL_NAME in MODELS.items():\n",
    "    OUTPUT_CSV = f\"chatbot_result/{scenario}/scenario_A_zero_shot_results_{key}.csv\"\n",
    "\n",
    "    print(f\"\\n🚀 Running Evaluation for: {MODEL_NAME}\")\n",
    "\n",
    "    result_df = pl.read_csv(OUTPUT_CSV)\n",
    "    preds = result_df[\"prediction\"].to_list()\n",
    "    golds = result_df[\"response\"].to_list()\n",
    "\n",
    "    summary = compute_metrics(preds, golds).with_columns(pl.lit(key).alias(\"Model\"))\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "# Combine all models (long)\n",
    "if all_summaries:\n",
    "    final_df = pl.concat(all_summaries).select([\"Model\", \"Metric\", \"Score\"])\n",
    "    final_df.write_csv(os.path.join(OUT_DIR, \"Scenario_A_Evaluation_AllModels.csv\"))\n",
    "\n",
    "    # Pivot for side-by-side comparison\n",
    "    pivot_df = final_df.pivot(index=\"Metric\", columns=\"Model\", values=\"Score\")\n",
    "    pivot_df.write_csv(os.path.join(OUT_DIR, \"Scenario_A_Evaluation_Pivot.csv\"))\n",
    "\n",
    "    print(\"\\n📊 Combined (pivot) comparison:\")\n",
    "    print(pivot_df)\n",
    "    print(\"\\n✅ Saved under 'Evaluation/'\")\n",
    "    print(\"- Scenario_A_Evaluation_AllModels.csv\")\n",
    "    print(\"- Scenario_A_Evaluation_Pivot.csv\")\n",
    "else:\n",
    "    print(\"⚠️ No model results were processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
