{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ba09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import random\n",
    "import emoji\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import markdown\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71035986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insert your token here\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc9f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636c6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21586\n",
      "20517\n",
      "14454\n",
      "shape: (11, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ category     ‚îÜ counts ‚îÇ\n",
      "‚îÇ ---          ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str          ‚îÜ u32    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ ACCOUNT      ‚îÜ 3251   ‚îÇ\n",
      "‚îÇ ORDER        ‚îÜ 2152   ‚îÇ\n",
      "‚îÇ REFUND       ‚îÜ 1527   ‚îÇ\n",
      "‚îÇ SHIPPING     ‚îÜ 1156   ‚îÇ\n",
      "‚îÇ DELIVERY     ‚îÜ 1102   ‚îÇ\n",
      "‚îÇ ‚Ä¶            ‚îÜ ‚Ä¶      ‚îÇ\n",
      "‚îÇ INVOICE      ‚îÜ 1076   ‚îÇ\n",
      "‚îÇ PAYMENT      ‚îÜ 1028   ‚îÇ\n",
      "‚îÇ FEEDBACK     ‚îÜ 1004   ‚îÇ\n",
      "‚îÇ CANCEL       ‚îÜ 539    ‚îÇ\n",
      "‚îÇ SUBSCRIPTION ‚îÜ 537    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "‚úÖ Split sizes:\n",
      "Train: 4154\n",
      "Val:   890\n",
      "Test:  893\n",
      "RAG:  5044\n",
      "\n",
      "üìä Category distribution in test set:\n",
      "shape: (4, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ category ‚îÜ count ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---   ‚îÇ\n",
      "‚îÇ str      ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ DELIVERY ‚îÜ 166   ‚îÇ\n",
      "‚îÇ ORDER    ‚îÜ 323   ‚îÇ\n",
      "‚îÇ REFUND   ‚îÜ 230   ‚îÇ\n",
      "‚îÇ SHIPPING ‚îÜ 174   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "\n",
    "# Data cleaning\n",
    "# fill in null\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"response\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"intent\")\n",
    "      .cast(str)\n",
    "      .fill_null(\"unknown\")\n",
    "])\n",
    "\n",
    "# Remove emoji\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Remove all emojis safely\n",
    "\n",
    "# Apply to instruction and response\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\").map_elements(remove_emojis).alias(\"instruction\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"response\").map_elements(remove_emojis).alias(\"response\")\n",
    "])\n",
    "\n",
    "#Exclude noisy flags\n",
    "# Filter out rows where 'flags' contains Z, Q, or W ===\n",
    "flag = [\"flags\"]\n",
    "df_z = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\")\n",
    ")\n",
    "\n",
    "df_zw = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\")\n",
    ")\n",
    "\n",
    "df_clean = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Q\")\n",
    ")\n",
    "\n",
    "print(df_z.height)\n",
    "print(df_zw.height)\n",
    "print(df_clean.height)\n",
    "\n",
    "category_counts = (\n",
    "    df_clean\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.count().alias(\"counts\"))\n",
    "    .sort(\"counts\", descending=True)\n",
    ")\n",
    "\n",
    "print(category_counts)\n",
    "\n",
    "# Filter only selected categories\n",
    "selected_categories = [\"ORDER\", \"REFUND\", \"SHIPPING\", \"DELIVERY\"]\n",
    "df_selected = df_clean.filter(\n",
    "    pl.col(\"category\").is_in(selected_categories)\n",
    ")\n",
    "\n",
    "\n",
    "# Split dataset by category\n",
    "# === Configuration ===\n",
    "LABEL_COL = \"category\"  # üîÅ Replace with \"intent\" or any stratification column\n",
    "SPLIT_RATIO_TRAIN = 0.7\n",
    "SPLIT_RATIO_VAL = 0.15\n",
    "SEED = 123\n",
    "df_final = df_selected.clone()\n",
    "\n",
    "# === Stratified split logic ===\n",
    "random.seed(SEED)\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "val_parts = []\n",
    "\n",
    "for label in df_final[LABEL_COL].unique().to_list():\n",
    "    group_df = df_final.filter(pl.col(LABEL_COL) == label)\n",
    "    group_df = group_df.sample(n=len(group_df), shuffle=True, seed=SEED)\n",
    "\n",
    "    n = len(group_df)\n",
    "    train_idx = int(n * SPLIT_RATIO_TRAIN)\n",
    "    val_idx = int(n * (SPLIT_RATIO_TRAIN + SPLIT_RATIO_VAL))\n",
    "\n",
    "    train_parts.append(group_df[:train_idx])\n",
    "    val_parts.append(group_df[train_idx:val_idx])\n",
    "    test_parts.append(group_df[val_idx:])\n",
    "\n",
    "# === Combine all groups\n",
    "train_df = pl.concat(train_parts).sort([\"category\", \"instruction\"])\n",
    "val_df = pl.concat(val_parts).sort([\"category\", \"instruction\"])\n",
    "test_df = pl.concat(test_parts).sort([\"category\", \"instruction\"])\n",
    "\n",
    "rag_data = pl.concat([train_df, val_df], how=\"vertical\")\n",
    "\n",
    "rag_data = rag_data.with_columns(\n",
    "    rag_data[\"intent\"]\n",
    "    .str.replace_all(\"_\", \" \")   # replace all underscores with spaces\n",
    "    .str.strip_chars()\n",
    "    .str.to_titlecase()          # Delivery Options Today\n",
    "    .alias(\"intent\")\n",
    ")\n",
    "print(\"‚úÖ Split sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(f\"RAG:  {len(rag_data)}\")\n",
    "\n",
    "print(\"\\nüìä Category distribution in test set:\")\n",
    "print(test_df.select([pl.col(LABEL_COL)]).to_series().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546b9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup LanceDB with FAQ dataset ---\n",
    "embedding_model = get_registry().get(\"colbert\").create(name=\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "class FAQModel(LanceModel):\n",
    "    \"\"\"Schema for FAQ vector table\"\"\"\n",
    "    text: str = embedding_model.SourceField()\n",
    "    vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()\n",
    "    question: str              # <-- add this\n",
    "    category: str\n",
    "    intent: str\n",
    "\n",
    "\n",
    "def create_faq_table(df):\n",
    "    \"\"\"\n",
    "    Create a LanceDB table from FAQ dataframe\n",
    "    \"\"\"\n",
    "    db = lancedb.connect(\"/home/zorin17/Desktop/LLM/\")\n",
    "    table = db.create_table(\n",
    "        \"LANCEDB_FAQ\",\n",
    "        schema=FAQModel,\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "    \n",
    "    # Combine question + answer into one text for embedding\n",
    "    entries = []\n",
    "    for row in df.iter_rows(named=True):   # Polars way\n",
    "        entry = {\n",
    "            \"text\": f\"Question: {row['instruction']}\\nAnswer: {row['response']}\",\n",
    "            \"question\": row[\"instruction\"],   # <-- extra column\n",
    "            \"category\": row[\"category\"],\n",
    "            \"intent\": row[\"intent\"],\n",
    "        }\n",
    "        entries.append(entry)\n",
    "\n",
    "    table.add(pd.DataFrame(entries))  # LanceDB expects pandas\n",
    "    return table\n",
    "\n",
    "# Create the LanceDB table (first time setup only)\n",
    "table = create_faq_table(rag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Lancedb table\n",
    "def load_faq_table():\n",
    "    \"\"\"\n",
    "    Load the existing LanceDB FAQ table\n",
    "    \"\"\"\n",
    "    db = lancedb.connect(\"/home/zorin17/Desktop/LLM/\")\n",
    "    table = db.open_table(\"LANCEDB_FAQ\")\n",
    "    return table\n",
    "\n",
    "\n",
    "table = load_faq_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32845242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- your models ----------\n",
    "generate_models = {\n",
    "    \"llama\":    \"meta-llama/Llama-3.2-1B\",\n",
    "    \"qwen\":     \"Qwen/Qwen3-0.6B-Base\",\n",
    "    \"olmo\":     \"allenai/OLMo-2-0425-1B\"\n",
    "}\n",
    "\n",
    "# choose model key\n",
    "MODEL_KEY = \"llama\"   # or \"llama\", \"deepseek\"\n",
    "MODEL_NAME = generate_models[MODEL_KEY]\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# load tokenizer + model once\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if USE_CUDA else torch.float32,\n",
    "    device_map=\"auto\"  # will use GPU if available\n",
    ")\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "custom_template = \"\"\"{% if messages | selectattr('role','equalto','system') | list %}\n",
    "System: {{ (messages | selectattr('role','equalto','system') | map(attribute='content') | list) | join('\\\\n') }}\n",
    "{% endif %}\n",
    "{% for m in messages %}\n",
    "{% if m['role'] == 'user' -%}\n",
    "User: {{ m['content'] }}\n",
    "{% elif m['role'] == 'assistant' -%}\n",
    "Assistant: {{ m['content'] }}\n",
    "{% elif m['role'] == 'tool' -%}\n",
    "Tool: {{ m['content'] }}\n",
    "{% elif m['role'] == 'developer' -%}\n",
    "System: {{ m['content'] }}\n",
    "{% else -%}\n",
    "{{ m['role']|capitalize }}: {{ m['content'] }}\n",
    "{% endif -%}\n",
    "{% endfor %}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# custom_template = \"\"\"{% for message in messages -%}\n",
    "# <|im_start|>{{ message['role'] }}\n",
    "# {{ message['content'] }}<|im_end|>\n",
    "# {% endfor -%}\n",
    "# {% if add_generation_prompt %}<|im_start|>assistant\n",
    "# {% endif %}\"\"\"\n",
    "\n",
    "\n",
    "# --- Search with optional filtering ---\n",
    "def search(query, table, top_k=5, category=None, intent=None):\n",
    "    search_obj = table.search(query).limit(top_k)\n",
    "\n",
    "    # Apply filters using `.where()`\n",
    "    if category:\n",
    "        search_obj = search_obj.where(f\"category = '{category}'\")\n",
    "    if intent:\n",
    "        search_obj = search_obj.where(f\"intent = '{intent}'\")\n",
    "\n",
    "    result = search_obj.to_list()\n",
    "\n",
    "    # Format context with citation numbering\n",
    "    contexts = []\n",
    "    for i, r in enumerate(result, 1):\n",
    "        contexts.append(f\"[{i}] {r['text']}\")\n",
    "    return \"\\n\".join(contexts)\n",
    "\n",
    "def generate(base_prompt, question, context, temperature=0.1, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    HF-only generator. Builds a single-string prompt and decodes ONLY new tokens\n",
    "    after the prompt to avoid echoing.\n",
    "    \"\"\"\n",
    "\n",
    "    system_content = f\"{base_prompt.format(question, context)}\"\n",
    "\n",
    "    # skip chat template to be fair for other two models\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContext: {context}\"}\n",
    "    ]\n",
    "\n",
    "    # tokenize and move to device\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # adds the assistant turn\n",
    "        chat_template = custom_template\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(hf_model.device)\n",
    "    \n",
    "    # inputs = tokenizer(system_content, return_tensors=\"pt\").to(hf_model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    # generate\n",
    "    outputs = hf_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # decode ONLY the newly generated tokens (exclude prompt)\n",
    "    new_tokens = outputs.sequences[0, input_len:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # markdown -> HTML (for your existing UI)\n",
    "    return markdown.markdown(response_text)\n",
    "\n",
    "\n",
    "# --- Full RAG ---\n",
    "def rag(question, table, base_prompt, temperature=0.2, category=None, intent=None):\n",
    "    context = search(question, table, top_k=5, category=category, intent=intent)\n",
    "    answer = generate(base_prompt, question, context, temperature)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be84b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Thank you for your question! I am a helpful assistant at a retail store. I can help you track your orders and provide you the most up-to-date information regarding your purchase. To track your purchases, you may visit our online store and navigate through the \"Order History\" or the \"Track Order\" section to enter your Order Number. If you have further questions or need assistance, feel Free to contact us!</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define prompt template\n",
    "base_prompt = \"\"\"You are a helpful retail assistant. Your task is to answer the user's question using the provided FAQ contexts. \n",
    "\n",
    "User question: {}\n",
    "Contexts:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# base_prompt = \"\"\"You are an AI assistant. Answer the user's question **only** using the provided FAQ contexts.\n",
    "\n",
    "# If the context does not contain the answer, reply exactly:\n",
    "# \"The provided context does not have the answer.\"\n",
    "\n",
    "# Question:\n",
    "# <<<{}>>>\n",
    "\n",
    "# Contexts (one or more snippets):\n",
    "# <<<{}>>>\n",
    "\n",
    "# Rules:\n",
    "# - Use information from the contexts only; do not invent facts.\n",
    "# - If multiple snippets are relevant, synthesize them briefly.\n",
    "# \"\"\"\n",
    "\n",
    "# Ask a question\n",
    "question = \"i want to track my order\"\n",
    "answer = rag(question, table, base_prompt, category=\"ORDER\")\n",
    "\n",
    "display(HTML(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865830e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dd595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
