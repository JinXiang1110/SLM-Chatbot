{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8ba09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zorin17/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import random\n",
    "import emoji\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import markdown\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71035986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Insert your token here\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc9f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636c6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21586\n",
      "20517\n",
      "14454\n",
      "shape: (11, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ category     â”† counts â”‚\n",
      "â”‚ ---          â”† ---    â”‚\n",
      "â”‚ str          â”† u32    â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ ACCOUNT      â”† 3251   â”‚\n",
      "â”‚ ORDER        â”† 2152   â”‚\n",
      "â”‚ REFUND       â”† 1527   â”‚\n",
      "â”‚ SHIPPING     â”† 1156   â”‚\n",
      "â”‚ DELIVERY     â”† 1102   â”‚\n",
      "â”‚ â€¦            â”† â€¦      â”‚\n",
      "â”‚ INVOICE      â”† 1076   â”‚\n",
      "â”‚ PAYMENT      â”† 1028   â”‚\n",
      "â”‚ FEEDBACK     â”† 1004   â”‚\n",
      "â”‚ CANCEL       â”† 539    â”‚\n",
      "â”‚ SUBSCRIPTION â”† 537    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "âœ… Split sizes:\n",
      "Train: 4154\n",
      "Val:   890\n",
      "Test:  893\n",
      "RAG:  5044\n",
      "\n",
      "ðŸ“Š Category distribution in test set:\n",
      "shape: (4, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ category â”† count â”‚\n",
      "â”‚ ---      â”† ---   â”‚\n",
      "â”‚ str      â”† u32   â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ DELIVERY â”† 166   â”‚\n",
      "â”‚ ORDER    â”† 323   â”‚\n",
      "â”‚ REFUND   â”† 230   â”‚\n",
      "â”‚ SHIPPING â”† 174   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
    "\n",
    "# Data cleaning\n",
    "# fill in null\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"response\")\n",
    "      .cast(str)\n",
    "      .str.to_lowercase()\n",
    "      .fill_null(\"\"),\n",
    "    \n",
    "    pl.col(\"intent\")\n",
    "      .cast(str)\n",
    "      .fill_null(\"unknown\")\n",
    "])\n",
    "\n",
    "# Remove emoji\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Remove all emojis safely\n",
    "\n",
    "# Apply to instruction and response\n",
    "df = df.with_columns([\n",
    "    pl.col(\"instruction\").map_elements(remove_emojis).alias(\"instruction\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"response\").map_elements(remove_emojis).alias(\"response\")\n",
    "])\n",
    "\n",
    "#Exclude noisy flags\n",
    "# Filter out rows where 'flags' contains Z, Q, or W ===\n",
    "flag = [\"flags\"]\n",
    "df_z = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\")\n",
    ")\n",
    "\n",
    "df_zw = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\")\n",
    ")\n",
    "\n",
    "df_clean = df.filter(\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Z\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"W\") &\n",
    "    ~pl.col(flag).cast(str).str.contains(\"Q\")\n",
    ")\n",
    "\n",
    "print(df_z.height)\n",
    "print(df_zw.height)\n",
    "print(df_clean.height)\n",
    "\n",
    "category_counts = (\n",
    "    df_clean\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.count().alias(\"counts\"))\n",
    "    .sort(\"counts\", descending=True)\n",
    ")\n",
    "\n",
    "print(category_counts)\n",
    "\n",
    "# Filter only selected categories\n",
    "selected_categories = [\"ORDER\", \"REFUND\", \"SHIPPING\", \"DELIVERY\"]\n",
    "df_selected = df_clean.filter(\n",
    "    pl.col(\"category\").is_in(selected_categories)\n",
    ")\n",
    "\n",
    "\n",
    "# Split dataset by category\n",
    "# === Configuration ===\n",
    "LABEL_COL = \"category\"  # ðŸ” Replace with \"intent\" or any stratification column\n",
    "SPLIT_RATIO_TRAIN = 0.7\n",
    "SPLIT_RATIO_VAL = 0.15\n",
    "SEED = 123\n",
    "df_final = df_selected.clone()\n",
    "\n",
    "# === Stratified split logic ===\n",
    "random.seed(SEED)\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "val_parts = []\n",
    "\n",
    "for label in df_final[LABEL_COL].unique().to_list():\n",
    "    group_df = df_final.filter(pl.col(LABEL_COL) == label)\n",
    "    group_df = group_df.sample(n=len(group_df), shuffle=True, seed=SEED)\n",
    "\n",
    "    n = len(group_df)\n",
    "    train_idx = int(n * SPLIT_RATIO_TRAIN)\n",
    "    val_idx = int(n * (SPLIT_RATIO_TRAIN + SPLIT_RATIO_VAL))\n",
    "\n",
    "    train_parts.append(group_df[:train_idx])\n",
    "    val_parts.append(group_df[train_idx:val_idx])\n",
    "    test_parts.append(group_df[val_idx:])\n",
    "\n",
    "# === Combine all groups\n",
    "train_df = pl.concat(train_parts).sort([\"category\", \"instruction\"])\n",
    "val_df = pl.concat(val_parts).sort([\"category\", \"instruction\"])\n",
    "test_df = pl.concat(test_parts).sort([\"category\", \"instruction\"])\n",
    "\n",
    "rag_data = pl.concat([train_df, val_df], how=\"vertical\")\n",
    "\n",
    "print(\"âœ… Split sizes:\")\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val:   {len(val_df)}\")\n",
    "print(f\"Test:  {len(test_df)}\")\n",
    "\n",
    "print(f\"RAG:  {len(rag_data)}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Category distribution in test set:\")\n",
    "print(test_df.select([pl.col(LABEL_COL)]).to_series().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546b9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Setup LanceDB with FAQ dataset ---\n",
    "# embedding_model = get_registry().get(\"colbert\").create(name=\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# class FAQModel(LanceModel):\n",
    "#     \"\"\"Schema for FAQ vector table\"\"\"\n",
    "#     text: str = embedding_model.SourceField()\n",
    "#     vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()\n",
    "#     category: str\n",
    "#     intent: str\n",
    "\n",
    "\n",
    "# def create_faq_table(df):\n",
    "#     \"\"\"\n",
    "#     Create a LanceDB table from FAQ dataframe\n",
    "#     \"\"\"\n",
    "#     db = lancedb.connect(\"/home/zorin17/Desktop/LLM/\")\n",
    "#     table = db.create_table(\n",
    "#         \"LANCEDB_FAQ\",\n",
    "#         schema=FAQModel,\n",
    "#         mode=\"overwrite\",\n",
    "#     )\n",
    "    \n",
    "#     # Combine question + answer into one text for embedding\n",
    "#     entries = []\n",
    "#     for row in df.iter_rows(named=True):   # Polars way\n",
    "#         entry = {\n",
    "#             \"text\": f\"Question: {row['instruction']}\\nAnswer: {row['response']}\",\n",
    "#             # \"text\": row['response'],\n",
    "#             \"category\": row[\"category\"],\n",
    "#             \"intent\": row[\"intent\"],\n",
    "#         }\n",
    "#         entries.append(entry)\n",
    "\n",
    "#     table.add(pd.DataFrame(entries))  # LanceDB expects pandas\n",
    "#     return table\n",
    "\n",
    "# # Create the LanceDB table (first time setup only)\n",
    "# table = create_faq_table(rag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Lancedb table\n",
    "def load_faq_table():\n",
    "    \"\"\"\n",
    "    Load the existing LanceDB FAQ table\n",
    "    \"\"\"\n",
    "    db = lancedb.connect(\"/home/zorin17/Desktop/LLM/\")\n",
    "    table = db.open_table(\"LANCEDB_FAQ\")\n",
    "    return table\n",
    "\n",
    "\n",
    "table = load_faq_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32845242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- your models ----------\n",
    "MODELS = {\n",
    "    \"llama\": {\n",
    "        \"base\":    \"meta-llama/Llama-3.2-1B\",\n",
    "        \"adapter\": \"qlora-outputs/Llama-3.2-1B-faq\",\n",
    "    },\n",
    "    \"qwen\": {\n",
    "        \"base\":    \"Qwen/Qwen3-0.6B-Base\",\n",
    "        \"adapter\": \"qlora-outputs/Qwen3-0.6B-Base-faq\",\n",
    "    },\n",
    "    \"olmo\": {\n",
    "        \"base\":    \"allenai/OLMo-2-0425-1B\",\n",
    "        \"adapter\": \"qlora-outputs/OLMo-2-0425-1B-faq\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# choose model key\n",
    "MODEL_KEY = \"qwen\"  # \"llama\" | \"qwen\" | \"olmo\"\n",
    "BASE_MODEL  = MODELS[MODEL_KEY][\"base\"]\n",
    "ADAPTER_DIR = MODELS[MODEL_KEY][\"adapter\"]\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# load tokenizer + model once\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- load base in 4-bit + attach LoRA adapter ---\n",
    "supports_bf16 = USE_CUDA and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if supports_bf16 else torch.float16,\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16 if supports_bf16 else torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "hf_model = PeftModel.from_pretrained(base, ADAPTER_DIR)  # <-- attach your QLoRA adapter\n",
    "# re-enable cache for inference\n",
    "if getattr(hf_model.config, \"use_cache\", None) is not True:\n",
    "    hf_model.config.use_cache = True\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "custom_template = \"\"\"{% if messages | selectattr('role','equalto','system') | list %}\n",
    "System: {{ (messages | selectattr('role','equalto','system') | map(attribute='content') | list) | join('\\\\n') }}\n",
    "{% endif %}\n",
    "{% for m in messages %}\n",
    "{% if m['role'] == 'user' -%}\n",
    "User: {{ m['content'] }}\n",
    "{% elif m['role'] == 'assistant' -%}\n",
    "Assistant: {{ m['content'] }}\n",
    "{% elif m['role'] == 'tool' -%}\n",
    "Tool: {{ m['content'] }}\n",
    "{% elif m['role'] == 'developer' -%}\n",
    "System: {{ m['content'] }}\n",
    "{% else -%}\n",
    "{{ m['role']|capitalize }}: {{ m['content'] }}\n",
    "{% endif -%}\n",
    "{% endfor %}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# custom_template = \"\"\"{% for message in messages -%}\n",
    "# <|im_start|>{{ message['role'] }}\n",
    "# {{ message['content'] }}<|im_end|>\n",
    "# {% endfor -%}\n",
    "# {% if add_generation_prompt %}<|im_start|>assistant\n",
    "# {% endif %}\"\"\"\n",
    "\n",
    "\n",
    "# --- Search with optional filtering ---\n",
    "def search(query, table, top_k=5, category=None, intent=None):\n",
    "    search_obj = table.search(query).limit(top_k)\n",
    "\n",
    "    # Apply filters using `.where()`\n",
    "    if category:\n",
    "        search_obj = search_obj.where(f\"category = '{category}'\")\n",
    "    if intent:\n",
    "        search_obj = search_obj.where(f\"intent = '{intent}'\")\n",
    "\n",
    "    result = search_obj.to_list()\n",
    "\n",
    "    if not result:\n",
    "        return \"[Context 1]:\\n(no relevant context found)\"\n",
    "    \n",
    "    # Format context with citation numbering\n",
    "    contexts = []\n",
    "    for i, r in enumerate(result, 1):\n",
    "        text = r.get(\"text\", \"\").strip()\n",
    "        contexts.append(f\"[Context {i}]:\\n{text}\\n\")   # <-- with label + line breaks\n",
    "    return \"\\n\".join(contexts)\n",
    "\n",
    "\n",
    "def generate(base_prompt, question, context, temperature=0.1, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    HF-only generator. Builds a single-string prompt and decodes ONLY new tokens\n",
    "    after the prompt to avoid echoing.\n",
    "    \"\"\"\n",
    "\n",
    "    system_content = f\"{base_prompt.format(question, context)}\"\n",
    "\n",
    "    # skip chat template to be fair for other two models\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContext: {context}\"}\n",
    "    ]\n",
    "\n",
    "    # tokenize and move to device\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # adds the assistant turn\n",
    "        chat_template = custom_template\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(hf_model.device)\n",
    "    \n",
    "    # inputs = tokenizer(system_content, return_tensors=\"pt\").to(hf_model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    # generate\n",
    "    outputs = hf_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # decode ONLY the newly generated tokens (exclude prompt)\n",
    "    new_tokens = outputs.sequences[0, input_len:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # markdown -> HTML (for your existing UI)\n",
    "    return markdown.markdown(response_text)\n",
    "\n",
    "\n",
    "# --- Full RAG ---\n",
    "def rag(question, table, base_prompt, temperature=0.0, category=None, intent=None):\n",
    "    context = search(question, table, top_k=5, category=category, intent=intent)\n",
    "    answer = generate(base_prompt, question, context, temperature)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08062d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_sentence_structure_multi_p(html_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Capitalize sentences inside multiple <p>...</p> blocks.\n",
    "    Handles:\n",
    "    - First letter of each paragraph\n",
    "    - After punctuation (.!?)\n",
    "    - After \"Step X:\"\n",
    "    - After numbered lists like \"1)\" or \"2.\"\n",
    "    - After colons (:)\n",
    "    - Title Case for all words inside quotes (\"...\" or '...')\n",
    "    \"\"\"\n",
    "\n",
    "    def title_case_inside_quotes(match):\n",
    "        # Take the quoted content and apply Title Case\n",
    "        content = match.group(2)\n",
    "        content_tc = \" \".join(w.capitalize() for w in content.split())\n",
    "        return match.group(1) + content_tc + match.group(3)\n",
    "\n",
    "    def fix_text(text: str) -> str:\n",
    "        text = text.strip()\n",
    "\n",
    "        # Capitalize very first letter of the paragraph\n",
    "        text = re.sub(r'^[a-z]', lambda m: m.group(0).upper(), text)\n",
    "\n",
    "        # Capitalize after punctuation (.!?)\n",
    "        text = re.sub(r'([.!?]\\s+)([a-z])',\n",
    "                      lambda m: m.group(1) + m.group(2).upper(),\n",
    "                      text)\n",
    "\n",
    "        # Capitalize after \"Step X:\"\n",
    "        text = re.sub(r'(Step\\s*\\d+:)(\\s*)([a-z])',\n",
    "                      lambda m: m.group(1) + m.group(2) + m.group(3).upper(),\n",
    "                      text,\n",
    "                      flags=re.IGNORECASE)\n",
    "\n",
    "        # Capitalize after numbered list \"1)\" or \"2.\"\n",
    "        text = re.sub(r'(\\d+[\\)\\.]\\s*)([a-z])',\n",
    "                      lambda m: m.group(1) + m.group(2).upper(),\n",
    "                      text)\n",
    "\n",
    "        # Capitalize after colon \":\"\n",
    "        text = re.sub(r'(:\\s*)([a-z])',\n",
    "                      lambda m: m.group(1) + m.group(2).upper(),\n",
    "                      text)\n",
    "\n",
    "        # Title Case inside double quotes\n",
    "        text = re.sub(r'(\")([^\"]+)(\")', title_case_inside_quotes, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def fix_paragraph(match):\n",
    "        inner = match.group(1).strip()\n",
    "        return f\"<p>{fix_text(inner)}</p>\"\n",
    "\n",
    "    return re.sub(r\"<p>(.*?)</p>\", fix_paragraph, html_text,\n",
    "                  flags=re.DOTALL | re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be84b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>I understand your desire to check your order and i'm ready to assist. To check up on your order status, you have several options available to you. Let me guide you through the process:</p>\n",
       "<p>1) Visit our official website: Go to our website's homepage and navigate to our \"Order Status\" or similar section. You should find a dedicated page where you can view the status updates of your recent orders.</p>\n",
       "<p>2) Use the order tracking feature: If you've already placed an order and have the order confirmation number, you should be able to access the order status on our official order tracking page. Simply enter the order reference number in the designated field and click \"Track Order\" to see the current progress.</p>\n",
       "<p>3) Contact customer support: If the above methods don't work or you're unsure about the process, you may want to reach us for further assistance. Our customer service team is available to provide personalized guidance and help you track your orders. You can contact us during our designated hours, typically from 8:30 a.m. To 5:00 p.m. Local time on our live chat support page at {{customer support phone number}} or through the live chat on our web page.</p>\n",
       "<p>I'm here for you to ensure you have all the information you need to track and check the current state of your orders efficiently. If there's anything else i can assist you or if you need any further clarification, please don't hesitate to let me hear. Your satisfaction and peace of mind are my top priorities.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define prompt template\n",
    "base_prompt = \"\"\"You are a helpful retail assistant. Your task is to answer the user question using provided contexts as the answer. \n",
    "You must make your response organized and structured.\n",
    "\n",
    "User question: {}\n",
    "Contexts:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# base_prompt = \"\"\"You are a helpful retail assistant. Your task is to answer the user question using exactly one of the provided contexts as the answer. \n",
    "# You must make your response organized and structured.\n",
    "\n",
    "# User question: {}\n",
    "# Contexts:\n",
    "# {}\n",
    "# \"\"\"\n",
    "\n",
    "# Ask a quesition\n",
    "question = \"i want to check my order\"\n",
    "answer = rag(question, table, base_prompt, category=\"ORDER\")\n",
    "\n",
    "display(HTML(enforce_sentence_structure_multi_p(answer)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
